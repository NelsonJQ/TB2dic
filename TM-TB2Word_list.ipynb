{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc56489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Language File Processor with Configurable Filtering\n",
    "# Supports Excel (.xlsx, .xls) and XLIFF (.xliff, .xlf, .xml) files\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Set, List, Tuple\n",
    "import html\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags and decode HTML entities, with space insertion for br/p tags\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # First, replace br and p tags with spaces to prevent word concatenation\n",
    "    # Handle both self-closing and regular br tags\n",
    "    text = re.sub(r'&lt;/?br\\s*/?&gt;', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'&lt;/?p\\s*/?&gt;', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'&lt;p\\s+[^&]*&gt;', ' ', text, flags=re.IGNORECASE)  # p with attributes\n",
    "    text = re.sub(r'&lt;/p&gt;', ' ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove other HTML tags (without space insertion)\n",
    "    text = re.sub(r'&lt;[^&]*&gt;', '', text)\n",
    "    \n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def matches_time_pattern(token: str) -> bool:\n",
    "    \"\"\"Check if token matches time pattern like 3PM, 10AM, 5PA, 12AL\"\"\"\n",
    "    return bool(re.match(r'^\\d+(PM|AM|PA|AL)$', token, re.IGNORECASE))\n",
    "\n",
    "def matches_digit_word_pattern(token: str) -> bool:\n",
    "    \"\"\"Check if token matches digit-word pattern like 123-neutral\"\"\"\n",
    "    return bool(re.match(r'^\\d+-\\w+$', token))\n",
    "\n",
    "def process_english_contractions(text: str) -> str:\n",
    "    \"\"\"Process English contractions while preserving case\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Comprehensive English contractions mapping\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\", \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\", \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\", \"she's\": \"she is\", \"shouldn't\": \"should not\", \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\", \"what's\": \"what is\", \"where's\": \"where is\", \"who's\": \"who is\",\n",
    "        \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\", \"you've\": \"you have\", \"'cause\": \"because\", \"how's\": \"how is\",\n",
    "        \"when's\": \"when is\", \"why's\": \"why is\", \"y'all\": \"you all\", \"would've\": \"would have\",\n",
    "        \"should've\": \"should have\", \"might've\": \"might have\", \"must've\": \"must have\"\n",
    "    }\n",
    "    \n",
    "    def replace_contraction(match):\n",
    "        contraction = match.group(0)\n",
    "        lower_contraction = contraction.lower()\n",
    "        \n",
    "        if lower_contraction in contractions:\n",
    "            replacement = contractions[lower_contraction]\n",
    "            \n",
    "            # Preserve case: if original was capitalized, capitalize the replacement\n",
    "            if contraction[0].isupper():\n",
    "                replacement = replacement.capitalize()\n",
    "            \n",
    "            return replacement\n",
    "        return contraction\n",
    "    \n",
    "    # Use word boundaries to match contractions\n",
    "    pattern = r\"\\b(?:\" + \"|\".join(re.escape(cont) for cont in contractions.keys()) + r\")\\b\"\n",
    "    result = re.sub(pattern, replace_contraction, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_portuguese_contractions(text: str) -> str:\n",
    "    \"\"\"Process Portuguese contractions and apostrophe patterns\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Handle apostrophe contractions like d'Água -> de Água\n",
    "    text = re.sub(r\"\\bd'([A-ZÁÉÍÓÚÂÊÔÀÇ])\", r\"de \\1\", text)\n",
    "    text = re.sub(r\"\\bl'([A-ZÁÉÍÓÚÂÊÔÀÇ])\", r\"le \\1\", text)\n",
    "    \n",
    "    # Handle hyphenated pronouns like amá-lo -> amar lo\n",
    "    text = re.sub(r\"([aeiouáéíóúâêôàç])-([lm][eoasá]s?)\\b\", r\"\\1r \\2\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def has_wip_markers(text: str) -> bool:\n",
    "    \"\"\"Check if text contains WIP/translation markers\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Pattern to match markers like {WIP}, [NOTRAD], [no trad], {no_trad}, etc.\n",
    "    pattern = r'[\\[\\{].*(wip|notrad|no trad|no_trad).*[\\]\\}]'\n",
    "    return bool(re.search(pattern, text, re.IGNORECASE))\n",
    "\n",
    "def tokenize_text(text: str, language: str = \"default\") -> Set[str]:\n",
    "    \"\"\"\n",
    "    Enhanced tokenize function with language-specific processing and comprehensive filtering\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to tokenize\n",
    "        language: Language for processing (\"english\", \"portuguese\", or \"default\")\n",
    "    \n",
    "    Returns:\n",
    "        Set of filtered tokens\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return set()\n",
    "    \n",
    "    # Step 1: Remove HTML tags and decode entities\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    # Step 2: Remove URLs and email addresses\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Step 3: Language-specific contraction processing\n",
    "    if language.lower() == \"english\":\n",
    "        text = process_english_contractions(text)\n",
    "    elif language.lower() == \"portuguese\":\n",
    "        text = process_portuguese_contractions(text)\n",
    "    # For \"default\" or other languages, skip contraction processing\n",
    "    \n",
    "   # Step 4: Enhanced punctuation (including º character)\n",
    "    basic_punct = '.,;:¡!?\"\"''()[]{}«»„\"‚-+=*/@#$%^&|\\\\<>~`º'\n",
    "    basic_punct += \"“”‘’\"  # Adding curly and single quotes\n",
    "    unicode_dashes = '\\u2014\\u2013'  # em-dash and en-dash\n",
    "    punctuation = basic_punct + unicode_dashes\n",
    "    \n",
    "    # Step 5: Tokenize by whitespace and punctuation, preserving internal hyphens and apostrophes\n",
    "    tokens = re.findall(r\"[^\\s\" + re.escape(punctuation) + r\"]+(?:[-'][^\\s\" + re.escape(punctuation) + r\"]+)*\", text)\n",
    "\n",
    "    \n",
    "    # Step 6: Clean and filter tokens\n",
    "    filtered_tokens = set()\n",
    "    for token in tokens:\n",
    "        # Remove leading/trailing apostrophes and hyphens\n",
    "        cleaned_token = token.strip(\"'-\")\n",
    "        \n",
    "        # Skip if empty after cleaning\n",
    "        if not cleaned_token:\n",
    "            continue\n",
    "        \n",
    "        # Skip short tokens (< 3 characters)\n",
    "        if len(cleaned_token) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Skip tokens that are chains of the same character\n",
    "        if len(set(cleaned_token.lower())) == 1:\n",
    "            continue\n",
    "        \n",
    "        # Skip tokens that are only digits\n",
    "        if cleaned_token.isdigit():\n",
    "            continue\n",
    "        \n",
    "        # Skip time patterns (e.g., \"3PM\", \"10AM\", \"5PA\", \"12AL\")\n",
    "        if matches_time_pattern(cleaned_token):\n",
    "            continue\n",
    "        \n",
    "        # Skip digit-word patterns (e.g., \"123-neutral\")\n",
    "        if matches_digit_word_pattern(cleaned_token):\n",
    "            continue\n",
    "        \n",
    "        filtered_tokens.add(cleaned_token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def detect_file_type(file_path: str) -> str:\n",
    "    \"\"\"Detect if file is Excel or XLIFF based on extension\"\"\"\n",
    "    file_path_lower = file_path.lower()\n",
    "    if file_path_lower.endswith(('.xlsx', '.xls')):\n",
    "        return 'excel'\n",
    "    elif file_path_lower.endswith(('.xliff', '.xlf', '.xml')):\n",
    "        return 'xliff'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type for: {file_path}\")\n",
    "\n",
    "def process_excel_file(file_path: str, language_code: str, ignore_identical_translation: bool, \n",
    "                      tokenize_language: str, skip_square_brackets: bool, skip_all_caps: bool, \n",
    "                      skip_wip_markers: bool) -> Tuple[Set[str], int, int]:\n",
    "    \"\"\"Process Excel file and extract tokens with configurable filtering\"\"\"\n",
    "    \n",
    "    # Try to find the sheet with actual data for the language\n",
    "    xl_file = pd.ExcelFile(file_path)\n",
    "    df = None\n",
    "    sheet_used = None\n",
    "    \n",
    "    for sheet_name in xl_file.sheet_names:\n",
    "        temp_df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        if language_code in temp_df.columns:\n",
    "            non_null_count = temp_df[language_code].notna().sum()\n",
    "            if non_null_count > 0:\n",
    "                df = temp_df\n",
    "                sheet_used = sheet_name\n",
    "                print(f\"Using sheet '{sheet_name}' with {non_null_count} {language_code} values\")\n",
    "                break\n",
    "    \n",
    "    if df is None:\n",
    "        # Fallback to default sheet\n",
    "        df = pd.read_excel(file_path)\n",
    "        sheet_used = \"default\"\n",
    "    \n",
    "    print(f\"Excel columns: {list(df.columns)}\")\n",
    "    print(f\"Sheet used: {sheet_used}\")\n",
    "    \n",
    "    if language_code not in df.columns:\n",
    "        raise ValueError(f\"Language code '{language_code}' not found in Excel columns: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"Total Excel rows to process: {len(df)}\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    tokens = set()\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    skip_reasons = {\"identical\": 0, \"square_brackets\": 0, \"all_caps\": 0, \"wip_markers\": 0, \"empty_target\": 0}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        source_text = str(row.iloc[1]) if len(row) > 1 else \"\"  # Assume source is second column\n",
    "        \n",
    "        # Check if target is NaN or empty BEFORE converting to string\n",
    "        target_value = row[language_code]\n",
    "        if pd.isna(target_value):\n",
    "            skipped_count += 1\n",
    "            skip_reasons[\"empty_target\"] += 1\n",
    "            continue\n",
    "            \n",
    "        target_text = str(target_value)\n",
    "        \n",
    "        # Skip if target is empty string after conversion\n",
    "        if target_text.strip() == '':\n",
    "            skipped_count += 1\n",
    "            skip_reasons[\"empty_target\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Apply filters based on configuration\n",
    "        should_skip = False\n",
    "        skip_reason = None\n",
    "        \n",
    "        # Filter 1: Identical translation\n",
    "        if ignore_identical_translation and source_text == target_text:\n",
    "            should_skip = True\n",
    "            skip_reason = \"identical\"\n",
    "        \n",
    "        # Filter 2: Square brackets in source\n",
    "        elif skip_square_brackets and re.search(r'\\[.+\\]', source_text):\n",
    "            should_skip = True\n",
    "            skip_reason = \"square_brackets\"\n",
    "        \n",
    "        # Filter 3: All caps target\n",
    "        elif skip_all_caps and target_text.isupper() and len(target_text) > 2:\n",
    "            should_skip = True\n",
    "            skip_reason = \"all_caps\"\n",
    "        \n",
    "        # Filter 4: WIP markers\n",
    "        elif skip_wip_markers and has_wip_markers(target_text):\n",
    "            should_skip = True\n",
    "            skip_reason = \"wip_markers\"\n",
    "        \n",
    "        if should_skip:\n",
    "            skipped_count += 1\n",
    "            skip_reasons[skip_reason] += 1\n",
    "            continue\n",
    "        \n",
    "        # Process the target text\n",
    "        processed_count += 1\n",
    "        text_tokens = tokenize_text(target_text, tokenize_language)\n",
    "        tokens.update(text_tokens)\n",
    "    \n",
    "    # Print skip statistics\n",
    "    print(f\"Skip reasons breakdown:\")\n",
    "    for reason, count in skip_reasons.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {reason}: {count}\")\n",
    "    \n",
    "    return tokens, processed_count, skipped_count\n",
    "\n",
    "def process_xliff_file(file_path: str, language_code: str, ignore_identical_translation: bool,\n",
    "                      tokenize_language: str, skip_square_brackets: bool, skip_all_caps: bool,\n",
    "                      skip_wip_markers: bool) -> Tuple[Set[str], int, int]:\n",
    "    \"\"\"Process XLIFF file and extract tokens with configurable filtering\"\"\"\n",
    "    \n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Find the namespace\n",
    "    namespace = ''\n",
    "    if root.tag.startswith('{'):\n",
    "        namespace = root.tag.split('}')[0] + '}'\n",
    "    \n",
    "    # Find file element and check language attributes\n",
    "    file_elem = root.find(f'.//{namespace}file')\n",
    "    if file_elem is None:\n",
    "        raise ValueError(\"No file element found in XLIFF\")\n",
    "    \n",
    "    source_lang = file_elem.get('source-language', '')\n",
    "    target_lang = file_elem.get('target-language', '')\n",
    "    \n",
    "    print(f\"XLIFF source language: {source_lang}\")\n",
    "    print(f\"XLIFF target language: {target_lang}\")\n",
    "    \n",
    "    # Determine if we should extract from source or target elements\n",
    "    use_source = (language_code == source_lang)\n",
    "    use_target = (language_code == target_lang)\n",
    "    \n",
    "    if not (use_source or use_target):\n",
    "        raise ValueError(f\"Language code '{language_code}' not found in XLIFF languages: {source_lang}, {target_lang}\")\n",
    "    \n",
    "    # Find all trans-unit elements\n",
    "    trans_units = root.findall(f'.//{namespace}trans-unit')\n",
    "    print(f\"Total XLIFF segments to process: {len(trans_units)}\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    tokens = set()\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    skip_reasons = {\"identical\": 0, \"square_brackets\": 0, \"all_caps\": 0, \"wip_markers\": 0}\n",
    "    \n",
    "    for trans_unit in trans_units:\n",
    "        source_elem = trans_unit.find(f'{namespace}source')\n",
    "        target_elem = trans_unit.find(f'{namespace}target')\n",
    "        \n",
    "        source_text = source_elem.text if source_elem is not None and source_elem.text else \"\"\n",
    "        target_text = target_elem.text if target_elem is not None and target_elem.text else \"\"\n",
    "        \n",
    "        # Determine which text to process\n",
    "        text_to_process = source_text if use_source else target_text\n",
    "        \n",
    "        # Skip if text is empty\n",
    "        if not text_to_process:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Apply filters based on configuration\n",
    "        should_skip = False\n",
    "        skip_reason = None\n",
    "        \n",
    "        # Filter 1: Identical translation (only relevant for target)\n",
    "        if ignore_identical_translation and use_target and source_text == target_text:\n",
    "            should_skip = True\n",
    "            skip_reason = \"identical\"\n",
    "        \n",
    "        # Filter 2: Square brackets in source\n",
    "        elif skip_square_brackets and re.search(r'\\[.+\\]', source_text):\n",
    "            should_skip = True\n",
    "            skip_reason = \"square_brackets\"\n",
    "        \n",
    "        # Filter 3: All caps target (only relevant for target)\n",
    "        elif skip_all_caps and use_target and target_text.isupper() and len(target_text) > 2:\n",
    "            should_skip = True\n",
    "            skip_reason = \"all_caps\"\n",
    "        \n",
    "        # Filter 4: WIP markers\n",
    "        elif skip_wip_markers and has_wip_markers(text_to_process):\n",
    "            should_skip = True\n",
    "            skip_reason = \"wip_markers\"\n",
    "        \n",
    "        if should_skip:\n",
    "            skipped_count += 1\n",
    "            skip_reasons[skip_reason] += 1\n",
    "            continue\n",
    "        \n",
    "        # Process the text\n",
    "        processed_count += 1\n",
    "        text_tokens = tokenize_text(text_to_process, tokenize_language)\n",
    "        tokens.update(text_tokens)\n",
    "    \n",
    "    # Print skip statistics\n",
    "    print(f\"Skip reasons breakdown:\")\n",
    "    for reason, count in skip_reasons.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {reason}: {count}\")\n",
    "    \n",
    "    return tokens, processed_count, skipped_count\n",
    "\n",
    "def export_tokens_to_txt(tokens: Set[str], output_path: str):\n",
    "    \"\"\"Export tokens to a text file, one per line, sorted alphabetically\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for token in sorted(tokens):\n",
    "            f.write(token + '\\n')\n",
    "    print(f\"Exported {len(tokens)} unique tokens to: {output_path}\")\n",
    "\n",
    "# Create sample files for demonstration\n",
    "def create_sample_xliff():\n",
    "    \"\"\"Create a sample XLIFF file for testing\"\"\"\n",
    "    sample_xliff_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<xliff version=\"1.2\" xmlns=\"urn:oasis:names:tc:xliff:document:1.2\">\n",
    "    <file datatype=\"plaintext\" original=\"sample\" source-language=\"fr-fr\" target-language=\"es-es\">\n",
    "        <body>\n",
    "            <trans-unit id=\"sample.1\">\n",
    "                <source>Votre alignement est probablement au sommet, vos ennemis n'existent plus à l'Apogée.</source>\n",
    "                <target>Tu alineamiento está probablemente en la cumbre, tus enemigos no existen en el Apogeo.</target>\n",
    "            </trans-unit>\n",
    "            <trans-unit id=\"sample.2\">\n",
    "                <source>Test avec des crochets [DEBUG] dans le source</source>\n",
    "                <target>Prueba con corchetes en el origen</target>\n",
    "            </trans-unit>\n",
    "        </body>\n",
    "    </file>\n",
    "</xliff>\"\"\"\n",
    "    \n",
    "    with open(\"sample.xliff\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(sample_xliff_content)\n",
    "    print(\"Sample XLIFF file created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc357496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path: str, language_code: str, output_path: str = None, \n",
    "                ignore_identical_translation: bool = True, tokenize_language: str = \"default\",\n",
    "                skip_square_brackets: bool = True, skip_all_caps: bool = True, \n",
    "                skip_wip_markers: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to process a file and extract tokens for a given language code\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or XLIFF file\n",
    "        language_code: Language code (e.g., \"es-es\")\n",
    "        output_path: Optional output path for the txt file\n",
    "        ignore_identical_translation: If True (default), skip entries where target equals source\n",
    "        tokenize_language: Language for tokenization processing (\"english\", \"portuguese\", or \"default\")\n",
    "        skip_square_brackets: If True (default), skip entries with square brackets in source\n",
    "        skip_all_caps: If True (default), skip entries with all-caps target text\n",
    "        skip_wip_markers: If True (default), skip entries with WIP/NOTRAD markers\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing started at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
    "    \n",
    "    # Print filter configuration\n",
    "    print(f\"\\nFilter configuration:\")\n",
    "    print(f\"  - Skip identical translations: {ignore_identical_translation}\")\n",
    "    print(f\"  - Skip square brackets: {skip_square_brackets}\")\n",
    "    print(f\"  - Skip all caps: {skip_all_caps}\")\n",
    "    print(f\"  - Skip WIP markers: {skip_wip_markers}\")\n",
    "    print(f\"  - Tokenization language: {tokenize_language}\")\n",
    "    \n",
    "    # Detect file type\n",
    "    file_type = detect_file_type(file_path)\n",
    "    print(f\"Detected file type: {file_type}\")\n",
    "    \n",
    "    # Process file based on type\n",
    "    if file_type == 'excel':\n",
    "        tokens, processed_count, skipped_count = process_excel_file(\n",
    "            file_path, language_code, ignore_identical_translation, tokenize_language,\n",
    "            skip_square_brackets, skip_all_caps, skip_wip_markers)\n",
    "        entry_type = \"rows\"\n",
    "    elif file_type == 'xliff':\n",
    "        tokens, processed_count, skipped_count = process_xliff_file(\n",
    "            file_path, language_code, ignore_identical_translation, tokenize_language,\n",
    "            skip_square_brackets, skip_all_caps, skip_wip_markers)\n",
    "        entry_type = \"segments\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    \n",
    "    # Calculate timing\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nProcessing completed at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
    "    print(f\"Total processing time: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
    "    print(f\"Processing statistics:\")\n",
    "    print(f\"  - Processed {entry_type}: {processed_count:,}\")\n",
    "    print(f\"  - Skipped {entry_type}: {skipped_count:,}\")\n",
    "    print(f\"  - Total {entry_type}: {processed_count + skipped_count:,}\")\n",
    "    if duration > 0:\n",
    "        print(f\"  - Processing rate: {(processed_count + skipped_count)/duration:.1f} {entry_type}/second\")\n",
    "    print(f\"  - Found {len(tokens):,} unique tokens for language: {language_code}\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_path is None:\n",
    "        base_name = Path(file_path).stem\n",
    "        output_path = f\"{base_name}_{language_code}_tokens.txt\"\n",
    "    \n",
    "    # Export tokens\n",
    "    export_tokens_to_txt(tokens, output_path)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0214a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing with Excel file and configurable filters:\n",
      "Sample Excel file with filter test cases created!\n",
      "Excel columns: ['key', 'en-us', 'es-es', 'pt-br', 'fr-fr']\n",
      "Sample data:\n",
      "                 key                           en-us                       es-es                       pt-br                 fr-fr\n",
      "         normal_text   I can't believe it's working!                ¡Hola mundo!   Texto normal em português     Bonjour le monde!\n",
      "            wip_test           This is {WIP} content  Este es contenido [NOTRAD]     Conteúdo {no_trad} aqui     Contenu {WIP} ici\n",
      "     square_brackets             Normal English text     Texto normal en español              Como vai você?  [Debug] texte normal\n",
      "            all_caps                   SHOUTING TEXT         TEXTO EN MAYÚSCULAS         TEXTO EM MAIÚSCULAS   TEXTE EN MAJUSCULES\n",
      "           identical                    Same content                Same content           Conteúdo idêntico     Conteúdo idêntico\n",
      "english_contractions We don't know what's happening. No sabemos qué está pasando Encontrei-me com d'Artagnan Texte français normal\n",
      "\n",
      "============================================================\n",
      "TESTING WIP MARKERS FILTER\n",
      "============================================================\n",
      "Testing WIP marker detection:\n",
      "'Normal text without markers' -> Has WIP markers: False\n",
      "'Text with {WIP} marker' -> Has WIP markers: True\n",
      "'Content [NOTRAD] here' -> Has WIP markers: True\n",
      "'Some {no trad} content' -> Has WIP markers: True\n",
      "'Text with [no_trad] marker' -> Has WIP markers: True\n",
      "'Mixed content {WIP} and more text' -> Has WIP markers: True\n",
      "'Text [WIP] in brackets' -> Has WIP markers: True\n",
      "\n",
      "============================================================\n",
      "TESTING CONFIGURABLE FILTERS\n",
      "============================================================\n",
      "\n",
      "1. Processing with ALL filters enabled:\n",
      "Processing started at: 2025-09-14 18:20:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: True\n",
      "  - Skip square brackets: True\n",
      "  - Skip all caps: True\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'es-es', 'pt-br', 'fr-fr']\n",
      "Total Excel rows to process: 6\n",
      "Skip reasons breakdown:\n",
      "  - identical: 1\n",
      "  - all_caps: 1\n",
      "  - wip_markers: 1\n",
      "\n",
      "Processing completed at: 2025-09-14 18:20:26\n",
      "Total processing time: 0.05 seconds (0.00 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 3\n",
      "  - Skipped rows: 3\n",
      "  - Total rows: 6\n",
      "  - Processing rate: 111.0 rows/second\n",
      "  - Found 9 unique tokens for language: es-es\n",
      "Exported 9 unique tokens to: tokens_all_filters.txt\n",
      "Tokens with all filters: ['Hola', 'Texto', 'español', 'está', 'mundo', 'normal', 'pasando', 'qué', 'sabemos']\n",
      "\n",
      "2. Processing with NO filters:\n",
      "Processing started at: 2025-09-14 18:20:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: False\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'es-es', 'pt-br', 'fr-fr']\n",
      "Total Excel rows to process: 6\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 18:20:26\n",
      "Total processing time: 0.03 seconds (0.00 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 6\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 6\n",
      "  - Processing rate: 227.5 rows/second\n",
      "  - Found 15 unique tokens for language: pt-br\n",
      "Exported 15 unique tokens to: tokens_no_filters.txt\n",
      "Tokens with no filters: ['Como', 'Conteúdo', 'Encontrei-me', 'MAIÚSCULAS', 'TEXTO', 'Texto', 'aqui', 'com', \"d'Artagnan\", 'idêntico', 'no_trad', 'normal', 'português', 'vai', 'você']\n",
      "\n",
      "3. Processing with ONLY WIP filter:\n",
      "Processing started at: 2025-09-14 18:20:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'es-es', 'pt-br', 'fr-fr']\n",
      "Total Excel rows to process: 6\n",
      "Skip reasons breakdown:\n",
      "  - wip_markers: 1\n",
      "\n",
      "Processing completed at: 2025-09-14 18:20:26\n",
      "Total processing time: 0.04 seconds (0.00 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 5\n",
      "  - Skipped rows: 1\n",
      "  - Total rows: 6\n",
      "  - Processing rate: 158.8 rows/second\n",
      "  - Found 13 unique tokens for language: pt-br\n",
      "Exported 13 unique tokens to: tokens_wip_only.txt\n",
      "Tokens with WIP filter only: ['Como', 'Conteúdo', 'Encontrei-me', 'MAIÚSCULAS', 'TEXTO', 'Texto', 'com', \"d'Artagnan\", 'idêntico', 'normal', 'português', 'vai', 'você']\n",
      "\n",
      "Tokens filtered out by all filters: ['Como', 'Conteúdo', 'Encontrei-me', 'MAIÚSCULAS', 'TEXTO', 'aqui', 'com', \"d'Artagnan\", 'idêntico', 'no_trad', 'português', 'vai', 'você']\n",
      "Tokens filtered out by WIP filter only: ['aqui', 'no_trad']\n",
      "\n",
      "============================================================\n",
      "TESTING ENGLISH WITH CONFIGURABLE FILTERS\n",
      "============================================================\n",
      "\n",
      "Processing Excel for en-us with English language processing and selective filters:\n",
      "Processing started at: 2025-09-14 18:20:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: True\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: True\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'es-es', 'pt-br', 'fr-fr']\n",
      "Total Excel rows to process: 6\n",
      "Skip reasons breakdown:\n",
      "  - identical: 6\n",
      "\n",
      "Processing completed at: 2025-09-14 18:20:26\n",
      "Total processing time: 0.02 seconds (0.00 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 0\n",
      "  - Skipped rows: 6\n",
      "  - Total rows: 6\n",
      "  - Processing rate: 265.9 rows/second\n",
      "  - Found 0 unique tokens for language: en-us\n",
      "Exported 0 unique tokens to: excel_english_selective.txt\n",
      "Extracted English tokens: []\n",
      "\n",
      "==================================================\n",
      "Cleaning up files...\n",
      "Removed: sample_filter_test.xlsx\n",
      "Removed: tokens_all_filters.txt\n",
      "Removed: tokens_no_filters.txt\n",
      "Removed: tokens_wip_only.txt\n",
      "Removed: excel_english_selective.txt\n",
      "\n",
      "All demonstrations completed successfully!\n",
      "\n",
      "SUMMARY:\n",
      "- The script can handle both Excel (.xlsx, .xls) and XLIFF (.xliff, .xlf, .xml) files\n",
      "- NEW: Configurable filtering with individual control over each filter\n",
      "- NEW: WIP marker detection for {WIP}, [NOTRAD], [no trad], [no_trad] patterns\n",
      "- NEW: Detailed skip statistics showing why entries were filtered\n",
      "- Language-specific contraction processing for English and Portuguese\n",
      "- Comprehensive timing and progress reporting\n",
      "\n",
      "Filter options:\n",
      "- ignore_identical_translation: Skip entries where target equals source\n",
      "- skip_square_brackets: Skip entries with square brackets in source\n",
      "- skip_all_caps: Skip entries with all-caps target text\n",
      "- skip_wip_markers: Skip entries with WIP/translation markers\n",
      "\n",
      "Usage examples:\n",
      "# All filters enabled (default)\n",
      "process_file('file.xlsx', 'es-es')\n",
      "\n",
      "# Selective filtering\n",
      "process_file('file.xlsx', 'es-es', skip_wip_markers=True, skip_all_caps=False)\n",
      "\n",
      "# No filtering\n",
      "process_file('file.xlsx', 'es-es', ignore_identical_translation=False,\n",
      "             skip_square_brackets=False, skip_all_caps=False, skip_wip_markers=False)\n"
     ]
    }
   ],
   "source": [
    "# Demonstration with Excel file and new configurable filtering\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with Excel file and configurable filters:\")\n",
    "\n",
    "# Create sample Excel data with various filter test cases\n",
    "sample_data = {\n",
    "    'key': ['normal_text', 'wip_test', 'square_brackets', 'all_caps', 'identical', 'english_contractions'],\n",
    "    'en-us': [\"I can't believe it's working!\", \"This is {WIP} content\", \"Normal English text\", \"SHOUTING TEXT\", \"Same content\", \"We don't know what's happening.\"],\n",
    "    'es-es': ['¡Hola mundo!', 'Este es contenido [NOTRAD]', 'Texto normal en español', 'TEXTO EN MAYÚSCULAS', 'Same content', 'No sabemos qué está pasando'],\n",
    "    'pt-br': [\"Texto normal em português\", \"Conteúdo {no_trad} aqui\", \"Como vai você?\", \"TEXTO EM MAIÚSCULAS\", \"Conteúdo idêntico\", \"Encontrei-me com d'Artagnan\"],\n",
    "    'fr-fr': ['Bonjour le monde!', 'Contenu {WIP} ici', '[Debug] texte normal', 'TEXTE EN MAJUSCULES', 'Conteúdo idêntico', 'Texte français normal']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.to_excel(\"sample_filter_test.xlsx\", index=False)\n",
    "print(\"Sample Excel file with filter test cases created!\")\n",
    "print(f\"Excel columns: {list(df.columns)}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING WIP MARKERS FILTER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test has_wip_markers function\n",
    "wip_test_cases = [\n",
    "    \"Normal text without markers\",\n",
    "    \"Text with {WIP} marker\",\n",
    "    \"Content [NOTRAD] here\", \n",
    "    \"Some {no trad} content\",\n",
    "    \"Text with [no_trad] marker\",\n",
    "    \"Mixed content {WIP} and more text\",\n",
    "    \"Text [WIP] in brackets\"\n",
    "]\n",
    "\n",
    "print(\"Testing WIP marker detection:\")\n",
    "for text in wip_test_cases:\n",
    "    has_wip = has_wip_markers(text)\n",
    "    print(f\"'{text}' -> Has WIP markers: {has_wip}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING CONFIGURABLE FILTERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with all filters enabled (default)\n",
    "print(f\"\\n1. Processing with ALL filters enabled:\")\n",
    "try:\n",
    "    tokens_all_filters = process_file(\"sample_filter_test.xlsx\", \"es-es\", \"tokens_all_filters.txt\", \n",
    "                                    ignore_identical_translation=True,\n",
    "                                    skip_square_brackets=True,\n",
    "                                    skip_all_caps=True,\n",
    "                                    skip_wip_markers=True)\n",
    "    print(f\"Tokens with all filters: {sorted(tokens_all_filters)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with no filters (process everything)\n",
    "print(f\"\\n2. Processing with NO filters:\")\n",
    "try:\n",
    "    tokens_no_filters = process_file(\"sample_filter_test.xlsx\", \"pt-br\", \"tokens_no_filters.txt\",\n",
    "                                   ignore_identical_translation=False,\n",
    "                                   skip_square_brackets=False,\n",
    "                                   skip_all_caps=False,\n",
    "                                   skip_wip_markers=False)\n",
    "    print(f\"Tokens with no filters: {sorted(tokens_no_filters)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with only WIP filter\n",
    "print(f\"\\n3. Processing with ONLY WIP filter:\")\n",
    "try:\n",
    "    tokens_wip_only = process_file(\"sample_filter_test.xlsx\", \"pt-br\", \"tokens_wip_only.txt\",\n",
    "                                 ignore_identical_translation=False,\n",
    "                                 skip_square_brackets=False,\n",
    "                                 skip_all_caps=False,\n",
    "                                 skip_wip_markers=True)\n",
    "    print(f\"Tokens with WIP filter only: {sorted(tokens_wip_only)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Show differences\n",
    "if 'tokens_all_filters' in locals() and 'tokens_no_filters' in locals():\n",
    "    filtered_out = tokens_no_filters - tokens_all_filters\n",
    "    print(f\"\\nTokens filtered out by all filters: {sorted(filtered_out)}\")\n",
    "\n",
    "if 'tokens_wip_only' in locals() and 'tokens_no_filters' in locals():\n",
    "    wip_filtered = tokens_no_filters - tokens_wip_only\n",
    "    print(f\"Tokens filtered out by WIP filter only: {sorted(wip_filtered)}\")\n",
    "\n",
    "# Test English processing with configurable filters\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING ENGLISH WITH CONFIGURABLE FILTERS\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    print(f\"\\nProcessing Excel for en-us with English language processing and selective filters:\")\n",
    "    tokens_excel_en = process_file(\"sample_filter_test.xlsx\", \"en-us\", \"excel_english_selective.txt\", \n",
    "                                 ignore_identical_translation=True,\n",
    "                                 tokenize_language=\"english\",\n",
    "                                 skip_square_brackets=False,  # Allow square brackets\n",
    "                                 skip_all_caps=True,          # Skip all caps\n",
    "                                 skip_wip_markers=True)       # Skip WIP markers\n",
    "    print(f\"Extracted English tokens: {sorted(tokens_excel_en)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Clean up all files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cleaning up files...\")\n",
    "files_to_remove = [\n",
    "    \"sample.xliff\", \"sample_filter_test.xlsx\", \n",
    "    \"spanish_tokens.txt\", \"french_tokens.txt\",\n",
    "    \"tokens_all_filters.txt\", \"tokens_no_filters.txt\", \"tokens_wip_only.txt\",\n",
    "    \"excel_english_selective.txt\"\n",
    "]\n",
    "\n",
    "for file in files_to_remove:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Removed: {file}\")\n",
    "\n",
    "print(\"\\nAll demonstrations completed successfully!\")\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"- The script can handle both Excel (.xlsx, .xls) and XLIFF (.xliff, .xlf, .xml) files\")\n",
    "print(\"- NEW: Configurable filtering with individual control over each filter\")\n",
    "print(\"- NEW: WIP marker detection for {WIP}, [NOTRAD], [no trad], [no_trad] patterns\")\n",
    "print(\"- NEW: Detailed skip statistics showing why entries were filtered\")\n",
    "print(\"- Language-specific contraction processing for English and Portuguese\")\n",
    "print(\"- Comprehensive timing and progress reporting\")\n",
    "print(\"\\nFilter options:\")\n",
    "print(\"- ignore_identical_translation: Skip entries where target equals source\")\n",
    "print(\"- skip_square_brackets: Skip entries with square brackets in source\")\n",
    "print(\"- skip_all_caps: Skip entries with all-caps target text\") \n",
    "print(\"- skip_wip_markers: Skip entries with WIP/translation markers\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"# All filters enabled (default)\")\n",
    "print(\"process_file('file.xlsx', 'es-es')\")\n",
    "print(\"\")\n",
    "print(\"# Selective filtering\")\n",
    "print(\"process_file('file.xlsx', 'es-es', skip_wip_markers=True, skip_all_caps=False)\")\n",
    "print(\"\")\n",
    "print(\"# No filtering\")\n",
    "print(\"process_file('file.xlsx', 'es-es', ignore_identical_translation=False,\")\n",
    "print(\"             skip_square_brackets=False, skip_all_caps=False, skip_wip_markers=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032a6f4",
   "metadata": {},
   "source": [
    "# Get word list from language file (TB excel or TM/project XLIFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05d0a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing started at: 2025-09-14 19:18:21\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Total Excel rows to process: 32609\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 32609\n",
      "\n",
      "Processing completed at: 2025-09-14 19:18:27\n",
      "Total processing time: 5.82 seconds (0.10 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 0\n",
      "  - Skipped rows: 32,609\n",
      "  - Total rows: 32,609\n",
      "  - Processing rate: 5603.6 rows/second\n",
      "  - Found 0 unique tokens for language: pt-br\n",
      "Exported 0 unique tokens to: output\\pt-br_TOUCH_tokens_20250914_191821.txt\n"
     ]
    }
   ],
   "source": [
    "LANGFILE_PATH = r\"C:\\Users\\Nelso\\Downloads\\2025-06-13_Retro_TB_as at 6 May 2024.xlsx\" # Excel file path (terminology base)\n",
    "LANGFILE_PATH = r\"TB_ANK_202507/2025.07.09_TOUCH.xlsx\"  # Path to the sample XLIFF file\n",
    "LANG_CODE = \"pt-br\"\n",
    "#EXPORT_PATH = r\"C:\\Users\\Nelso\\Downloads\\spanishTB_tokens.txt\"\n",
    "EXPORT_FOLDER = \"output\"\n",
    "\n",
    "tokenization_lang = \"default\"  if LANG_CODE[:2] not in [\"en\", \"pt\"] else (\"english\" if LANG_CODE[:2] == \"en\" else \"portuguese\")\n",
    "\n",
    "if not os.path.exists(EXPORT_FOLDER):\n",
    "    os.makedirs(EXPORT_FOLDER)\n",
    "\n",
    "time_stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXPORT_PATH = os.path.join(EXPORT_FOLDER, f\"{LANG_CODE}_TOUCH_tokens_{time_stamp}.txt\")\n",
    "# Process the sample file for Spanish (es-es)\n",
    "try:\n",
    "    tokens = process_file(LANGFILE_PATH, LANG_CODE, EXPORT_PATH, ignore_identical_translation=False,\n",
    "                          tokenize_language=tokenization_lang, skip_square_brackets=False, skip_all_caps=False, skip_wip_markers=True)\n",
    "    #print(f\"\\nExtracted tokens: {sorted(tokens)}\")\n",
    "    \n",
    "    # Show the content of the output file\n",
    "    #with open(\"spanish_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "     #   content = f.read()\n",
    "    #print(f\"\\nContent of spanish_tokens.txt:\\n{content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a2639",
   "metadata": {},
   "source": [
    "## Batch processing - Get word list from all suppported files from folder\n",
    "Languages to process : EN, PT, ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05cf5c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 xlsx files to process\n",
      "Target language codes: ['pt-br', 'pt-BR', 'en-us', 'en-gb', 'en-GB', 'es-es', 'es-ES', 'en-US']\n",
      "======================================================================\n",
      "\n",
      "📁 Processing file: 2023.03.15_ONE_MORE_GATE_TB.xlsx\n",
      "🎮 Extracted game name: ONE\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:21:11\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-br not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:21:11\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-BR not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:21:12\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 432 en-us values\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 432\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 19:21:12\n",
      "Total processing time: 0.17 seconds (0.00 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 432\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 432\n",
      "  - Processing rate: 2538.0 rows/second\n",
      "  - Found 359 unique tokens for language: en-us\n",
      "Exported 359 unique tokens to: output\\en-us_ONE_tokens_20250914_192112.txt\n",
      "  ✅ Successfully processed en-us: 359 tokens exported to en-us_ONE_tokens_20250914_192112.txt\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:21:12\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-gb not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:21:12\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-GB not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:21:12\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 432 es-es values\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 432\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 19:21:12\n",
      "Total processing time: 0.15 seconds (0.00 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 432\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 432\n",
      "  - Processing rate: 2909.3 rows/second\n",
      "  - Found 365 unique tokens for language: es-es\n",
      "Exported 365 unique tokens to: output\\es-es_ONE_tokens_20250914_192112.txt\n",
      "  ✅ Successfully processed es-es: 365 tokens exported to es-es_ONE_tokens_20250914_192112.txt\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:21:12\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code es-ES not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:21:12\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'en-us', 'fr-fr', 'zh-cn', 'de-de', 'es-es']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-US not found in file columns - skipping\n",
      "\n",
      "📁 Processing file: 2025-01-08_WAVEN_TB.xlsx\n",
      "🎮 Extracted game name: WAVEN\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:21:13\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-br not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:21:16\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Using sheet 'Data_Glossaire' with 8349 pt-BR values\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: Data_Glossaire\n",
      "Total Excel rows to process: 8361\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 12\n",
      "\n",
      "Processing completed at: 2025-09-14 19:21:19\n",
      "Total processing time: 2.84 seconds (0.05 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 8,349\n",
      "  - Skipped rows: 12\n",
      "  - Total rows: 8,361\n",
      "  - Processing rate: 2940.2 rows/second\n",
      "  - Found 4,275 unique tokens for language: pt-BR\n",
      "Exported 4275 unique tokens to: output\\pt-br_WAVEN_tokens_20250914_192116.txt\n",
      "  ✅ Successfully processed pt-BR: 4275 tokens exported to pt-br_WAVEN_tokens_20250914_192116.txt\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:21:19\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-us not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:21:22\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-gb not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:21:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-GB not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:21:29\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code es-es not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:21:33\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Using sheet 'Data_Glossaire' with 8349 es-ES values\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: Data_Glossaire\n",
      "Total Excel rows to process: 8361\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 12\n",
      "\n",
      "Processing completed at: 2025-09-14 19:21:36\n",
      "Total processing time: 2.62 seconds (0.04 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 8,349\n",
      "  - Skipped rows: 12\n",
      "  - Total rows: 8,361\n",
      "  - Processing rate: 3191.7 rows/second\n",
      "  - Found 4,417 unique tokens for language: es-ES\n",
      "Exported 4417 unique tokens to: output\\es-es_WAVEN_tokens_20250914_192133.txt\n",
      "  ✅ Successfully processed es-ES: 4417 tokens exported to es-es_WAVEN_tokens_20250914_192133.txt\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:21:36\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Data_Glossaire' with 8349 en-US values\n",
      "Excel columns: ['Note', 'de-DE', 'de-DE (info)', 'en-US', 'en-US (info)', 'fr-FR', 'fr-FR (info)', 'pt-BR', 'pt-BR (info)', 'es-ES', 'es-ES (info)']\n",
      "Sheet used: Data_Glossaire\n",
      "Total Excel rows to process: 8361\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 12\n",
      "\n",
      "Processing completed at: 2025-09-14 19:21:39\n",
      "Total processing time: 3.44 seconds (0.06 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 8,349\n",
      "  - Skipped rows: 12\n",
      "  - Total rows: 8,361\n",
      "  - Processing rate: 2432.1 rows/second\n",
      "  - Found 3,974 unique tokens for language: en-US\n",
      "Exported 3974 unique tokens to: output\\en-us_WAVEN_tokens_20250914_192136.txt\n",
      "  ✅ Successfully processed en-US: 3974 tokens exported to en-us_WAVEN_tokens_20250914_192136.txt\n",
      "\n",
      "📁 Processing file: 2025-06-13_Retro_TB_as at 6 May 2024.xlsx\n",
      "🎮 Extracted game name: Retro\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:21:39\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Using sheet 'Sheet1' with 21357 pt-br values\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: Sheet1\n",
      "Total Excel rows to process: 21669\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 312\n",
      "\n",
      "Processing completed at: 2025-09-14 19:21:45\n",
      "Total processing time: 5.70 seconds (0.09 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 21,357\n",
      "  - Skipped rows: 312\n",
      "  - Total rows: 21,669\n",
      "  - Processing rate: 3803.7 rows/second\n",
      "  - Found 12,006 unique tokens for language: pt-br\n",
      "Exported 12006 unique tokens to: output\\pt-br_Retro_tokens_20250914_192139.txt\n",
      "  ✅ Successfully processed pt-br: 12006 tokens exported to pt-br_Retro_tokens_20250914_192139.txt\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:21:45\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-BR not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:21:51\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-us not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:21:56\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Sheet1' with 21570 en-gb values\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: Sheet1\n",
      "Total Excel rows to process: 21669\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 99\n",
      "\n",
      "Processing completed at: 2025-09-14 19:22:04\n",
      "Total processing time: 7.35 seconds (0.12 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 21,570\n",
      "  - Skipped rows: 99\n",
      "  - Total rows: 21,669\n",
      "  - Processing rate: 2949.5 rows/second\n",
      "  - Found 11,516 unique tokens for language: en-gb\n",
      "Exported 11516 unique tokens to: output\\en-gb_Retro_tokens_20250914_192156.txt\n",
      "  ✅ Successfully processed en-gb: 11516 tokens exported to en-gb_Retro_tokens_20250914_192156.txt\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:22:04\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-GB not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:22:10\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Using sheet 'Sheet1' with 21534 es-es values\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: Sheet1\n",
      "Total Excel rows to process: 21669\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 135\n",
      "\n",
      "Processing completed at: 2025-09-14 19:22:16\n",
      "Total processing time: 6.14 seconds (0.10 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 21,534\n",
      "  - Skipped rows: 135\n",
      "  - Total rows: 21,669\n",
      "  - Processing rate: 3527.6 rows/second\n",
      "  - Found 12,696 unique tokens for language: es-es\n",
      "Exported 12696 unique tokens to: output\\es-es_Retro_tokens_20250914_192210.txt\n",
      "  ✅ Successfully processed es-es: 12696 tokens exported to es-es_Retro_tokens_20250914_192210.txt\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:22:16\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code es-ES not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:22:21\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['ID', 'fr-fr', 'en-gb', 'es-es', 'pt-br', 'Note']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-US not found in file columns - skipping\n",
      "\n",
      "📁 Processing file: 2025.07.09_TOUCH.xlsx\n",
      "🎮 Extracted game name: TOUCH\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:22:27\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 27879 pt-br values\n",
      "Excel columns: ['Key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 27883\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 4\n",
      "\n",
      "Processing completed at: 2025-09-14 19:22:40\n",
      "Total processing time: 12.92 seconds (0.22 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 27,879\n",
      "  - Skipped rows: 4\n",
      "  - Total rows: 27,883\n",
      "  - Processing rate: 2157.7 rows/second\n",
      "  - Found 18,233 unique tokens for language: pt-br\n",
      "Exported 18233 unique tokens to: output\\pt-br_TOUCH_tokens_20250914_192227.txt\n",
      "  ✅ Successfully processed pt-br: 18233 tokens exported to pt-br_TOUCH_tokens_20250914_192227.txt\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:22:40\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-BR not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:22:57\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-us not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:23:13\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Combine Sheet' with 32608 en-gb values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: Combine Sheet\n",
      "Total Excel rows to process: 32609\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 1\n",
      "\n",
      "Processing completed at: 2025-09-14 19:23:26\n",
      "Total processing time: 13.56 seconds (0.23 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 32,608\n",
      "  - Skipped rows: 1\n",
      "  - Total rows: 32,609\n",
      "  - Processing rate: 2404.3 rows/second\n",
      "  - Found 16,754 unique tokens for language: en-gb\n",
      "Exported 16754 unique tokens to: output\\en-gb_TOUCH_tokens_20250914_192313.txt\n",
      "  ✅ Successfully processed en-gb: 16754 tokens exported to en-gb_TOUCH_tokens_20250914_192313.txt\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:23:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-GB not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:23:42\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Using sheet 'Combine Sheet' with 32605 es-es values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: Combine Sheet\n",
      "Total Excel rows to process: 32609\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 4\n",
      "\n",
      "Processing completed at: 2025-09-14 19:23:52\n",
      "Total processing time: 10.78 seconds (0.18 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 32,605\n",
      "  - Skipped rows: 4\n",
      "  - Total rows: 32,609\n",
      "  - Processing rate: 3025.6 rows/second\n",
      "  - Found 19,312 unique tokens for language: es-es\n",
      "Exported 19312 unique tokens to: output\\es-es_TOUCH_tokens_20250914_192342.txt\n",
      "  ✅ Successfully processed es-es: 19312 tokens exported to es-es_TOUCH_tokens_20250914_192342.txt\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:23:52\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code es-ES not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:24:07\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-US not found in file columns - skipping\n",
      "\n",
      "📁 Processing file: 2025.07.28_DOFUS.xlsx\n",
      "🎮 Extracted game name: DOFUS\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:24:22\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 55664 pt-br values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 55678\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 14\n",
      "\n",
      "Processing completed at: 2025-09-14 19:24:40\n",
      "Total processing time: 17.46 seconds (0.29 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 55,664\n",
      "  - Skipped rows: 14\n",
      "  - Total rows: 55,678\n",
      "  - Processing rate: 3189.3 rows/second\n",
      "  - Found 26,339 unique tokens for language: pt-br\n",
      "Exported 26339 unique tokens to: output\\pt-br_DOFUS_tokens_20250914_192422.txt\n",
      "  ✅ Successfully processed pt-br: 26339 tokens exported to pt-br_DOFUS_tokens_20250914_192422.txt\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:24:40\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-BR not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:24:58\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-us not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:25:15\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 55678 en-gb values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 55678\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 19:25:35\n",
      "Total processing time: 19.90 seconds (0.33 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 55,678\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 55,678\n",
      "  - Processing rate: 2798.4 rows/second\n",
      "  - Found 24,038 unique tokens for language: en-gb\n",
      "Exported 24038 unique tokens to: output\\en-gb_DOFUS_tokens_20250914_192515.txt\n",
      "  ✅ Successfully processed en-gb: 24038 tokens exported to en-gb_DOFUS_tokens_20250914_192515.txt\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:25:35\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-GB not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:25:52\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 55678 es-es values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 55678\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 19:26:08\n",
      "Total processing time: 15.46 seconds (0.26 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 55,678\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 55,678\n",
      "  - Processing rate: 3602.2 rows/second\n",
      "  - Found 28,712 unique tokens for language: es-es\n",
      "Exported 28712 unique tokens to: output\\es-es_DOFUS_tokens_20250914_192552.txt\n",
      "  ✅ Successfully processed es-es: 28712 tokens exported to es-es_DOFUS_tokens_20250914_192552.txt\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:26:08\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code es-ES not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:26:26\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-US not found in file columns - skipping\n",
      "\n",
      "📁 Processing file: 2025.07.28_WAKFU.xlsx\n",
      "🎮 Extracted game name: WAKFU\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:26:45\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 29095 pt-br values\n",
      "Excel columns: ['Key', 'fr-fr', 'en-us', 'es-es', 'pt-br']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 29095\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 19:26:59\n",
      "Total processing time: 13.49 seconds (0.22 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 29,095\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 29,095\n",
      "  - Processing rate: 2156.4 rows/second\n",
      "  - Found 14,839 unique tokens for language: pt-br\n",
      "Exported 14839 unique tokens to: output\\pt-br_WAKFU_tokens_20250914_192645.txt\n",
      "  ✅ Successfully processed pt-br: 14839 tokens exported to pt-br_WAKFU_tokens_20250914_192645.txt\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "Processing started at: 2025-09-14 19:26:59\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: portuguese\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code pt-BR not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:27:16\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Result Sheet 1' with 29094 en-us values\n",
      "Excel columns: ['Key', 'fr-fr', 'en-us', 'es-es', 'pt-br']\n",
      "Sheet used: Result Sheet 1\n",
      "Total Excel rows to process: 29095\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 1\n",
      "\n",
      "Processing completed at: 2025-09-14 19:27:31\n",
      "Total processing time: 15.62 seconds (0.26 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 29,094\n",
      "  - Skipped rows: 1\n",
      "  - Total rows: 29,095\n",
      "  - Processing rate: 1862.6 rows/second\n",
      "  - Found 13,225 unique tokens for language: en-us\n",
      "Exported 13225 unique tokens to: output\\en-us_WAKFU_tokens_20250914_192716.txt\n",
      "  ✅ Successfully processed en-us: 13225 tokens exported to en-us_WAKFU_tokens_20250914_192716.txt\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:27:31\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Using sheet 'Combine Sheet' with 42035 en-gb values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: Combine Sheet\n",
      "Total Excel rows to process: 42036\n",
      "Skip reasons breakdown:\n",
      "  - empty_target: 1\n",
      "\n",
      "Processing completed at: 2025-09-14 19:27:47\n",
      "Total processing time: 15.78 seconds (0.26 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 42,035\n",
      "  - Skipped rows: 1\n",
      "  - Total rows: 42,036\n",
      "  - Processing rate: 2664.4 rows/second\n",
      "  - Found 13,225 unique tokens for language: en-gb\n",
      "Exported 13225 unique tokens to: output\\en-gb_WAKFU_tokens_20250914_192731.txt\n",
      "  ✅ Successfully processed en-gb: 13225 tokens exported to en-gb_WAKFU_tokens_20250914_192731.txt\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "Processing started at: 2025-09-14 19:27:47\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-GB not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:28:03\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Using sheet 'Combine Sheet' with 42036 es-es values\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: Combine Sheet\n",
      "Total Excel rows to process: 42036\n",
      "Skip reasons breakdown:\n",
      "\n",
      "Processing completed at: 2025-09-14 19:28:13\n",
      "Total processing time: 10.40 seconds (0.17 minutes)\n",
      "Processing statistics:\n",
      "  - Processed rows: 42,036\n",
      "  - Skipped rows: 0\n",
      "  - Total rows: 42,036\n",
      "  - Processing rate: 4041.2 rows/second\n",
      "  - Found 15,852 unique tokens for language: es-es\n",
      "Exported 15852 unique tokens to: output\\es-es_WAKFU_tokens_20250914_192803.txt\n",
      "  ✅ Successfully processed es-es: 15852 tokens exported to es-es_WAKFU_tokens_20250914_192803.txt\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "Processing started at: 2025-09-14 19:28:13\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: default\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code es-ES not found in file columns - skipping\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "Processing started at: 2025-09-14 19:28:27\n",
      "\n",
      "Filter configuration:\n",
      "  - Skip identical translations: False\n",
      "  - Skip square brackets: False\n",
      "  - Skip all caps: False\n",
      "  - Skip WIP markers: True\n",
      "  - Tokenization language: english\n",
      "Detected file type: excel\n",
      "Excel columns: ['key', 'fr-fr', 'en-gb', 'es-es', 'de-de', 'it-it', 'pt-br']\n",
      "Sheet used: default\n",
      "  ⏭️  Language code en-US not found in file columns - skipping\n",
      "\n",
      "📁 Processing file: ~$2025.07.09_TOUCH.xlsx\n",
      "🎮 Extracted game name: TOUCH\n",
      "\n",
      "  🌐 Trying language code: pt-br (normalized: pt-br)\n",
      "  ❌ Unexpected error processing pt-br: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: pt-BR (normalized: pt-br)\n",
      "  ❌ Unexpected error processing pt-BR: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: en-us (normalized: en-us)\n",
      "  ❌ Unexpected error processing en-us: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: en-gb (normalized: en-gb)\n",
      "  ❌ Unexpected error processing en-gb: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: en-GB (normalized: en-gb)\n",
      "  ❌ Unexpected error processing en-GB: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: es-es (normalized: es-es)\n",
      "  ❌ Unexpected error processing es-es: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: es-ES (normalized: es-es)\n",
      "  ❌ Unexpected error processing es-ES: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "  🌐 Trying language code: en-US (normalized: en-us)\n",
      "  ❌ Unexpected error processing en-US: File not found: TB_ANK_202507\\~$2025.07.09_TOUCH.xlsx\n",
      "\n",
      "======================================================================\n",
      "📊 PROCESSING SUMMARY\n",
      "======================================================================\n",
      "Total files found: 7\n",
      "Total language processing attempts: 56\n",
      "Successful exports: 18\n",
      "Errors encountered: 8\n",
      "Skipped (language not found): 30\n",
      "\n",
      "📂 Output files saved to: output/\n",
      "🎯 Next step: Use the dictionary filtering cell to remove common words\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "FOLDER_PATH = \"TB_ANK_202507\"\n",
    "TARGET_LANG_CODES = [\"pt-br\", \"pt-BR\", \"en-us\", \"en-gb\", \"en-GB\", \"es-es\", \"es-ES\", \"en-US\"]  # Add other languages as needed\n",
    "EXPORT_FOLDER = \"output\"\n",
    "\n",
    "def extract_game_name(filename: str) -> str:\n",
    "    \"\"\"Extract game name from filename after first underscore until next underscore or dot\"\"\"\n",
    "    # Remove file extension first\n",
    "    name_without_ext = Path(filename).stem\n",
    "    \n",
    "    # Split by underscore and get the second part (index 1)\n",
    "    parts = name_without_ext.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        # Get second part and clean it up (remove any spaces or special chars that might cause issues)\n",
    "        game_name = parts[1].replace(' ', '_').replace('-', '_')\n",
    "        return game_name\n",
    "    return \"unknown\"\n",
    "\n",
    "def normalize_language_code(lang_code: str) -> str:\n",
    "    \"\"\"Normalize language codes to standard format\"\"\"\n",
    "    # Convert to lowercase and replace underscores with hyphens\n",
    "    normalized = lang_code.lower().replace('_', '-')\n",
    "    return normalized\n",
    "\n",
    "def get_tokenization_language(lang_code: str) -> str:\n",
    "    \"\"\"Determine tokenization language based on language code\"\"\"\n",
    "    lang_prefix = lang_code[:2].lower()\n",
    "    if lang_prefix == \"en\":\n",
    "        return \"english\"\n",
    "    elif lang_prefix == \"pt\":\n",
    "        return \"portuguese\"\n",
    "    else:\n",
    "        return \"default\"\n",
    "\n",
    "def process_all_xlsx_files():\n",
    "    \"\"\"Process all xlsx files in the folder for all target language codes\"\"\"\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(EXPORT_FOLDER):\n",
    "        os.makedirs(EXPORT_FOLDER)\n",
    "    \n",
    "    # Get all xlsx files in the folder\n",
    "    xlsx_files = glob.glob(os.path.join(FOLDER_PATH, \"*.xlsx\"))\n",
    "    \n",
    "    if not xlsx_files:\n",
    "        print(f\"No xlsx files found in folder: {FOLDER_PATH}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(xlsx_files)} xlsx files to process\")\n",
    "    print(f\"Target language codes: {TARGET_LANG_CODES}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Track overall statistics\n",
    "    total_processed = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    # Process each file\n",
    "    for xlsx_file in xlsx_files:\n",
    "        filename = os.path.basename(xlsx_file)\n",
    "        game_name = extract_game_name(filename)\n",
    "        \n",
    "        print(f\"\\n📁 Processing file: {filename}\")\n",
    "        print(f\"🎮 Extracted game name: {game_name}\")\n",
    "        \n",
    "        # Try each target language code\n",
    "        for lang_code in TARGET_LANG_CODES:\n",
    "            normalized_lang = normalize_language_code(lang_code)\n",
    "            tokenization_lang = get_tokenization_language(normalized_lang)\n",
    "            \n",
    "            print(f\"\\n  🌐 Trying language code: {lang_code} (normalized: {normalized_lang})\")\n",
    "            \n",
    "            try:\n",
    "                # Generate timestamped export path with game name\n",
    "                time_stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                export_filename = f\"{normalized_lang}_{game_name}_tokens_{time_stamp}.txt\"\n",
    "                export_path = os.path.join(EXPORT_FOLDER, export_filename)\n",
    "                \n",
    "                # Skip if file already exists ignoring timestamp\n",
    "                export_filename_no_timestamp = f\"{normalized_lang}_{game_name}_tokens\"\n",
    "                regexp_pattern = re.compile(rf\"{re.escape(export_filename_no_timestamp)}_\\d{{8}}_\\d{{6}}\\.txt\")\n",
    "                existing_files = [f for f in os.listdir(EXPORT_FOLDER) if regexp_pattern.match(f)]\n",
    "                if existing_files:\n",
    "                    print(f\"  ⏭️  Output file already exists: {export_filename} - skipping\")\n",
    "                    continue\n",
    "                # Process the file\n",
    "                tokens = process_file(\n",
    "                    xlsx_file, \n",
    "                    lang_code,  # Use original language code for column matching\n",
    "                    export_path,\n",
    "                    ignore_identical_translation=False,\n",
    "                    tokenize_language=tokenization_lang,\n",
    "                    skip_square_brackets=False,\n",
    "                    skip_all_caps=False,\n",
    "                    skip_wip_markers=True\n",
    "                )\n",
    "                \n",
    "                print(f\"  ✅ Successfully processed {lang_code}: {len(tokens)} tokens exported to {export_filename}\")\n",
    "                total_processed += 1\n",
    "                \n",
    "            except ValueError as e:\n",
    "                if \"not found in Excel columns\" in str(e):\n",
    "                    print(f\"  ⏭️  Language code {lang_code} not found in file columns - skipping\")\n",
    "                else:\n",
    "                    print(f\"  ❌ Error processing {lang_code}: {e}\")\n",
    "                    total_errors += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Unexpected error processing {lang_code}: {e}\")\n",
    "                total_errors += 1\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 PROCESSING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total files found: {len(xlsx_files)}\")\n",
    "    print(f\"Total language processing attempts: {len(xlsx_files) * len(TARGET_LANG_CODES)}\")\n",
    "    print(f\"Successful exports: {total_processed}\")\n",
    "    print(f\"Errors encountered: {total_errors}\")\n",
    "    print(f\"Skipped (language not found): {len(xlsx_files) * len(TARGET_LANG_CODES) - total_processed - total_errors}\")\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        print(f\"\\n📂 Output files saved to: {EXPORT_FOLDER}/\")\n",
    "        print(\"🎯 Next step: Use the dictionary filtering cell to remove common words\")\n",
    "\n",
    "# Run the batch processing\n",
    "process_all_xlsx_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc19924",
   "metadata": {},
   "source": [
    "# Merge both token files\n",
    "\n",
    "Output : single list merged from the TB list + TM list.\n",
    "Purpose: Useful to avoid problematic non-translations in the TM (élément_FR, élément[WIP]_ES), and add the curated non-translation terms from the terminology base (Wabbit_FR = Wabbit_ES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb44d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_PATH1 = r\"C:\\Users\\Nelso\\Downloads\\spanishTB_tokens.txt\" #from TB\n",
    "TXT_PATH2 = r\"C:\\Users\\Nelso\\Downloads\\spanish_tokens.txt\" #from TM\n",
    "# Merge two text files into one with unique tokens\n",
    "def merge_token_files(file1: str, file2: str, output_file: str):\n",
    "    \"\"\"Merge two token files into one, ensuring unique tokens\"\"\"\n",
    "    if not os.path.exists(file1) or not os.path.exists(file2):\n",
    "        raise FileNotFoundError(\"One or both token files do not exist.\")\n",
    "    \n",
    "    tokens = set()\n",
    "    \n",
    "    # Read first file\n",
    "    with open(file1, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens.add(line.strip())\n",
    "    \n",
    "    # Read second file\n",
    "    with open(file2, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens.add(line.strip())\n",
    "    \n",
    "    # Write unique tokens to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for token in sorted(tokens):\n",
    "            f.write(token + '\\n')\n",
    "    \n",
    "    print(f\"Merged {len(tokens)} unique tokens into: {output_file}\")\n",
    "\n",
    "# Merge the two token files\n",
    "merge_token_files(TXT_PATH1, TXT_PATH2, r\"C:\\Users\\Nelso\\Downloads\\merged_spanish_tokens.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb837d6",
   "metadata": {},
   "source": [
    "# Filter words appearing in a common language dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbf6c8",
   "metadata": {},
   "source": [
    "## Filtering v2.0\n",
    "This new algorithm includes morphological patterns of the AFF files to improve the matching rules and remove more common language words from the Ankama dictionary.\n",
    "* Hunspell resources : https://hunspell.memoq.com/\n",
    "* AFF (affix morphological patterns) documentation : https://manpages.ubuntu.com/manpages/focal/man5/hunspell.5.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8705d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Hunspell .dic file for language 'es-es' not found in paths: {'es': 'dics\\\\es_dic\\\\es\\\\es_ES.dic', 'fr': 'dics\\\\fr_dic\\\\fr_FR.dic', 'pt': 'dics\\\\pt_dic\\\\pt_BR\\\\pt_BR.dic', 'en': 'dics\\\\en_dic\\\\en_GB.dic'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m PATH_Hunspell_dic \u001b[38;5;241m=\u001b[39m dic_lang_paths\u001b[38;5;241m.\u001b[39mget(LANG_CODE[:\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# Get the first two letters (e.g., 'es' from 'es-es')\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PATH_Hunspell_dic \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(PATH_Hunspell_dic):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHunspell .dic file for language \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLANG_CODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in paths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdic_lang_paths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m AFF_FILE_PATH \u001b[38;5;241m=\u001b[39m dic_lang_paths\u001b[38;5;241m.\u001b[39mget(LANG_CODE[:\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.dic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.aff\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m dic_lang_paths\u001b[38;5;241m.\u001b[39mget(LANG_CODE[:\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Path to .aff file\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Replace 'tokens' with 'filtered_tokens' and add timestamp in input PATH_Ankama_tokens\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Hunspell .dic file for language 'es-es' not found in paths: {'es': 'dics\\\\es_dic\\\\es\\\\es_ES.dic', 'fr': 'dics\\\\fr_dic\\\\fr_FR.dic', 'pt': 'dics\\\\pt_dic\\\\pt_BR\\\\pt_BR.dic', 'en': 'dics\\\\en_dic\\\\en_GB.dic'}"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Set, Dict, List, Tuple\n",
    "LANG_CODE = \"es-es\"  # Language code to process\n",
    "\n",
    "PATH_Ankama_tokens = \"output/es-es_TOUCH_tokens_20250914_192342.txt\"  # Path to the Ankama tokens file\n",
    "#PATH_Ankama_tokens = EXPORT_PATH  # Use the previously generated tokens file\n",
    "\n",
    "DIC_FOLDER = \"dics\"\n",
    "dic_lang_paths = {\n",
    "    # es : os path + dic folder + es + es_ES.dic\n",
    "    \"es\": os.path.join(DIC_FOLDER, \"es_dic\", \"es\", \"es_ES.dic\"),\n",
    "    \"fr\": os.path.join(DIC_FOLDER, \"fr_dic\", \"fr_FR.dic\"),\n",
    "    \"pt\": os.path.join(DIC_FOLDER, \"pt_dic\", \"pt_BR\", \"pt_BR.dic\"),\n",
    "    \"en\": os.path.join(DIC_FOLDER, \"en_dic\", \"en_GB.dic\")\n",
    "}\n",
    "\n",
    "# Define Hunspell dic based on LANG_CODE\n",
    "PATH_Hunspell_dic = dic_lang_paths.get(LANG_CODE[:2])  # Get the first two letters (e.g., 'es' from 'es-es')\n",
    "if not PATH_Hunspell_dic or not os.path.exists(PATH_Hunspell_dic):\n",
    "    raise FileNotFoundError(f\"Hunspell .dic file for language '{LANG_CODE}' not found in paths: {dic_lang_paths}\")\n",
    "\n",
    "AFF_FILE_PATH = dic_lang_paths.get(LANG_CODE[:2]).replace('.dic', '.aff') if dic_lang_paths.get(LANG_CODE[:2]) else None  # Path to .aff file\n",
    "\n",
    "# Replace 'tokens' with 'filtered_tokens' and add timestamp in input PATH_Ankama_tokens\n",
    "if 'tokens' in PATH_Ankama_tokens:\n",
    "    FILTERED_OUTPUT_PATH = PATH_Ankama_tokens.replace('tokens', f'filtered_tokens')\n",
    "else:\n",
    "    FILTERED_OUTPUT_PATH = Path(PATH_Ankama_tokens).stem + '_filtered_tokens.txt'\n",
    "\n",
    "def parse_aff_file(aff_file_path: str) -> Dict:\n",
    "    \"\"\"Parse Hunspell .aff file and extract affix rules\"\"\"\n",
    "    affixes = {'PFX': {}, 'SFX': {}}\n",
    "    \n",
    "    with open(aff_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_affix = None\n",
    "    current_type = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "            \n",
    "        # Parse prefix/suffix header definitions (e.g., \"PFX a Y 2\")\n",
    "        if parts[0] in ['PFX', 'SFX'] and len(parts) >= 3:\n",
    "            affix_type = parts[0]\n",
    "            flag = parts[1]\n",
    "            cross_product = parts[2] == 'Y'\n",
    "            \n",
    "            # Check if this is a header line (has count) or rule line\n",
    "            if len(parts) >= 4:\n",
    "                try:\n",
    "                    # Try to parse as count - if successful, this is a header line\n",
    "                    count = int(parts[3])\n",
    "                    # This is a header line\n",
    "                    if flag not in affixes[affix_type]:\n",
    "                        affixes[affix_type][flag] = {\n",
    "                            'cross_product': cross_product,\n",
    "                            'rules': []\n",
    "                        }\n",
    "                    current_affix = flag\n",
    "                    current_type = affix_type\n",
    "                    continue\n",
    "                except ValueError:\n",
    "                    # Not a number, so this is a rule line\n",
    "                    pass\n",
    "            \n",
    "            # Parse affix rule: PFX/SFX flag strip add condition\n",
    "            if len(parts) >= 4 and current_affix == flag and current_type == affix_type:\n",
    "                strip = parts[2] if parts[2] != '0' else ''\n",
    "                add = parts[3] if parts[3] != '0' else ''\n",
    "                condition = parts[4] if len(parts) > 4 else '.'\n",
    "                \n",
    "                if current_affix in affixes[current_type]:\n",
    "                    affixes[current_type][current_affix]['rules'].append({\n",
    "                        'strip': strip,\n",
    "                        'add': add,\n",
    "                        'condition': condition\n",
    "                    })\n",
    "    \n",
    "    return affixes\n",
    "\n",
    "def condition_matches(word: str, condition: str, is_prefix: bool = True) -> bool:\n",
    "    \"\"\"Check if word matches the affix condition pattern\"\"\"\n",
    "    if condition == '.':\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        if is_prefix:\n",
    "            # For prefixes, check the beginning of the word\n",
    "            return bool(re.match(f'^{condition}', word))\n",
    "        else:\n",
    "            # For suffixes, check the end of the word\n",
    "            return bool(re.search(f'{condition}$', word))\n",
    "    except re.error:\n",
    "        # If regex fails, do simple string matching\n",
    "        if is_prefix:\n",
    "            return word.startswith(condition.replace('[^', '').replace(']', ''))\n",
    "        else:\n",
    "            return word.endswith(condition.replace('[^', '').replace(']', ''))\n",
    "\n",
    "def generate_word_forms(base_word: str, flags: str, affixes: Dict) -> Set[str]:\n",
    "    \"\"\"Generate all possible word forms using affix rules\"\"\"\n",
    "    word_forms = {base_word}  # Always include the base word\n",
    "    \n",
    "    if not flags:\n",
    "        return word_forms\n",
    "    \n",
    "    # Process each flag character\n",
    "    for flag in flags:\n",
    "        # Apply prefixes\n",
    "        if flag in affixes['PFX']:\n",
    "            prefix_rules = affixes['PFX'][flag]['rules']\n",
    "            for rule in prefix_rules:\n",
    "                if condition_matches(base_word, rule['condition'], is_prefix=True):\n",
    "                    # Apply prefix rule\n",
    "                    if rule['strip']:\n",
    "                        if base_word.startswith(rule['strip']):\n",
    "                            modified_word = rule['add'] + base_word[len(rule['strip']):]\n",
    "                            word_forms.add(modified_word)\n",
    "                    else:\n",
    "                        modified_word = rule['add'] + base_word\n",
    "                        word_forms.add(modified_word)\n",
    "        \n",
    "        # Apply suffixes\n",
    "        if flag in affixes['SFX']:\n",
    "            suffix_rules = affixes['SFX'][flag]['rules']\n",
    "            for rule in suffix_rules:\n",
    "                if condition_matches(base_word, rule['condition'], is_prefix=False):\n",
    "                    # Apply suffix rule\n",
    "                    if rule['strip']:\n",
    "                        if base_word.endswith(rule['strip']):\n",
    "                            modified_word = base_word[:-len(rule['strip'])] + rule['add']\n",
    "                            word_forms.add(modified_word)\n",
    "                    else:\n",
    "                        modified_word = base_word + rule['add']\n",
    "                        word_forms.add(modified_word)\n",
    "    \n",
    "    return word_forms\n",
    "\n",
    "def filter_tokens_by_dictionary_with_affixes(txt_file_path: str, dic_file_path: str, aff_file_path: str, output_dic_path: str):\n",
    "    \"\"\"\n",
    "    Enhanced version that uses Hunspell affix rules for better matching\n",
    "    \n",
    "    Args:\n",
    "        txt_file_path: Path to the txt file with tokens (one per line)\n",
    "        dic_file_path: Path to the dic file (first line is token count, rest are tokens)\n",
    "        aff_file_path: Path to the .aff file with affix rules\n",
    "        output_dic_path: Path where the filtered dic file will be saved\n",
    "    \"\"\"\n",
    "    if not os.path.exists(txt_file_path):\n",
    "        raise FileNotFoundError(f\"Token file not found: {txt_file_path}\")\n",
    "    \n",
    "    if not os.path.exists(dic_file_path):\n",
    "        raise FileNotFoundError(f\"Dictionary file not found: {dic_file_path}\")\n",
    "        \n",
    "    if not os.path.exists(aff_file_path):\n",
    "        raise FileNotFoundError(f\"Affix file not found: {aff_file_path}\")\n",
    "    \n",
    "    # Parse affix rules\n",
    "    print(f\"Parsing affix rules from: {aff_file_path}\")\n",
    "    affixes = parse_aff_file(aff_file_path)\n",
    "    prefix_count = sum(len(rules['rules']) for rules in affixes['PFX'].values())\n",
    "    suffix_count = sum(len(rules['rules']) for rules in affixes['SFX'].values())\n",
    "    print(f\"Loaded {len(affixes['PFX'])} prefix flags ({prefix_count} rules) and {len(affixes['SFX'])} suffix flags ({suffix_count} rules)\")\n",
    "    \n",
    "    # Read tokens from txt file - preserve original case\n",
    "    print(f\"Reading tokens from: {txt_file_path}\")\n",
    "    original_txt_tokens = []  # Keep original case\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token = line.strip()\n",
    "            if token:\n",
    "                original_txt_tokens.append(token)  # Preserve original case\n",
    "    \n",
    "    print(f\"Loaded {len(original_txt_tokens)} tokens from txt file\")\n",
    "    \n",
    "    # Read dictionary file and generate all word forms\n",
    "    print(f\"Reading dictionary and generating word forms from: {dic_file_path}\")\n",
    "    with open(dic_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if not lines:\n",
    "        raise ValueError(\"Dictionary file is empty\")\n",
    "    \n",
    "    # First line is the token count\n",
    "    original_count = lines[0].strip()\n",
    "    print(f\"Dictionary token count: {original_count}\")\n",
    "    \n",
    "    # Generate all possible word forms from dictionary (in lowercase for matching)\n",
    "    all_dictionary_forms = set()\n",
    "    processed_entries = 0\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        processed_entries += 1\n",
    "        if processed_entries % 1000 == 0:\n",
    "            print(f\"Processed {processed_entries} dictionary entries...\")\n",
    "        \n",
    "        # Parse dictionary entry\n",
    "        if '/' in line:\n",
    "            base_word, flags = line.split('/', 1)\n",
    "        else:\n",
    "            base_word, flags = line, ''\n",
    "        \n",
    "        # Generate all word forms for this base word (lowercase for matching)\n",
    "        word_forms = generate_word_forms(base_word.lower(), flags, affixes)\n",
    "        all_dictionary_forms.update(word_forms)\n",
    "    \n",
    "    print(f\"Generated {len(all_dictionary_forms)} unique word forms from {processed_entries} dictionary entries\")\n",
    "    \n",
    "    # Filter txt tokens - remove those that match any dictionary form\n",
    "    # Compare lowercase versions but keep original case for output\n",
    "    filtered_tokens = []\n",
    "    removed_count = 0\n",
    "    sample_removals = []\n",
    "    \n",
    "    for original_token in original_txt_tokens:  # Use original case tokens\n",
    "        if original_token.lower() in all_dictionary_forms:  # Compare with lowercase\n",
    "            removed_count += 1\n",
    "            if len(sample_removals) < 10:\n",
    "                sample_removals.append(original_token)  # Show original case in samples\n",
    "        else:\n",
    "            filtered_tokens.append(original_token)  # Keep original case\n",
    "    \n",
    "    # Show some examples of removed tokens\n",
    "    if sample_removals:\n",
    "        print(f\"Sample removed tokens: {', '.join(sample_removals[:5])}{'...' if len(sample_removals) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"Removed {removed_count} tokens that match dictionary word forms\")\n",
    "    print(f\"Remaining tokens: {len(filtered_tokens)}\")\n",
    "    \n",
    "    # Write filtered tokens as dictionary file (preserving original case)\n",
    "    with open(output_dic_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(len(filtered_tokens)) + '\\n')\n",
    "        for token in filtered_tokens:  # These already have original case\n",
    "            f.write(token + '\\n')\n",
    "    \n",
    "    print(f\"Filtered tokens saved as dictionary to: {output_dic_path}\")\n",
    "    \n",
    "    return {\n",
    "        'original_txt_tokens': len(original_txt_tokens),\n",
    "        'dictionary_base_words': processed_entries,\n",
    "        'generated_word_forms': len(all_dictionary_forms),\n",
    "        'removed_tokens': removed_count,\n",
    "        'remaining_tokens': len(filtered_tokens)\n",
    "    }\n",
    "\n",
    "# Test the enhanced function\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING ENHANCED DICTIONARY FILTERING WITH AFFIX RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "if os.path.exists(AFF_FILE_PATH):\n",
    "    try:\n",
    "        result = filter_tokens_by_dictionary_with_affixes(\n",
    "            PATH_Ankama_tokens,      # txt file with tokens to filter\n",
    "            PATH_Hunspell_dic,    # dic file\n",
    "            AFF_FILE_PATH,           # aff file with rules\n",
    "            FILTERED_OUTPUT_PATH\n",
    "        )\n",
    "        \n",
    "        print(\"\\nENHANCED FILTERING RESULTS:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Original txt tokens: {result['original_txt_tokens']}\")\n",
    "        print(f\"Dictionary base words: {result['dictionary_base_words']}\")\n",
    "        print(f\"Generated word forms: {result['generated_word_forms']}\")\n",
    "        print(f\"Removed tokens: {result['removed_tokens']}\")\n",
    "        print(f\"Remaining tokens: {result['remaining_tokens']}\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = result['generated_word_forms'] - result['dictionary_base_words']\n",
    "        print(f\"Affix expansion factor: {result['generated_word_forms'] / result['dictionary_base_words']:.2f}x\")\n",
    "        print(f\"Additional word forms from affixes: {improvement}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"Affix file not found: {AFF_FILE_PATH}\")\n",
    "    print(\"Please provide the correct path to the .aff file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b92cf0",
   "metadata": {},
   "source": [
    "# Enhanced Language File Processor - Complete Summary\n",
    "\n",
    "## Features\n",
    "\n",
    "The script now includes **comprehensive filtering** with multiple advanced conditions to ensure high-quality token extraction.\n",
    "\n",
    "### Supported File Types\n",
    "- **Excel files** (`.xlsx`, `.xls`): Language code as column name\n",
    "- **XLIFF files** (`.xliff`, `.xlf`, `.xml`): Language code in `source-language` or `target-language` attributes\n",
    "\n",
    "### Key Functionality\n",
    "1. **File Type Detection**: Automatically detects file type based on extension\n",
    "2. **Language Matching**: \n",
    "   - Excel: Extracts from column matching the language code\n",
    "   - XLIFF: Extracts from `<source>` or `<target>` elements based on language attributes\n",
    "\n",
    "### **COMPREHENSIVE Filtering System**\n",
    "3. **Square Bracket Filtering**: Ignores entries where source text contains `[.+]` pattern\n",
    "4. **Target = Source Filtering**: Ignores entries where target text equals source text\n",
    "5. **All-Caps Target Filtering**: **NEW** - Ignores entries where target text is entirely in uppercase\n",
    "6. **HTML Tag Removal**: **NEW** - Removes HTML tags and decodes HTML entities before tokenization\n",
    "7. **Hyperlink & Email Removal**: Removes URLs and email addresses before tokenization\n",
    "8. **Token Edge Cleaning**: **NEW** - Removes leading/trailing apostrophes and hyphens from tokens\n",
    "9. **Short Token Filtering**: Removes tokens with length < 3 characters\n",
    "10. **Same Character Chain Filtering**: Removes tokens that are chains of the same character (e.g., \"aaa\", \"zzZZzz\")\n",
    "11. **Number-Only Token Filtering**: **NEW** - Removes tokens that consist only of digits\n",
    "12. **Time Pattern Filtering**: **NEW** - Removes tokens matching `\\d+(PA|PM|AM|AL)` pattern\n",
    "13. **Digit-Word Pattern Filtering**: **NEW** - Removes tokens matching `\\d+-\\w+` pattern (e.g., \"123-neutral\")\n",
    "14. **Enhanced Punctuation**: **NEW** - Includes º character in punctuation list\n",
    "15. **Tokenization**: Splits by whitespace and punctuation, preserving hyphens (`-`) and apostrophes (`'`)\n",
    "16. **Export**: Saves unique tokens (case-sensitive) to text file, one per line\n",
    "\n",
    "### Usage\n",
    "```python\n",
    "# Basic usage\n",
    "tokens = process_file(file_path, language_code)\n",
    "\n",
    "# With custom output path\n",
    "tokens = process_file(file_path, language_code, output_path)\n",
    "```\n",
    "\n",
    "### Example Advanced Filtering Results\n",
    "**Input Processing:**\n",
    "- ✅ **\"Hola mundo\"** → `['Hola', 'mundo']`\n",
    "- ❌ **\"[Debug] test\"** → Skipped (square brackets in source)\n",
    "- ❌ **\"Same text\"** → Skipped (target equals source)\n",
    "- ❌ **\"TODO EN MAYÚSCULAS\"** → Skipped (all caps target)\n",
    "- ✅ **HTML content** → Tags removed, entities decoded\n",
    "- ✅ **\"'Resistencia 'Robo'\"** → `['Resistencia', 'Robo']` (edges cleaned)\n",
    "- ❌ **Number tokens: \"123\", \"456\"** → Filtered out (numbers only)\n",
    "- ❌ **Time patterns: \"3PM\", \"10AM\"** → Filtered out (time pattern)\n",
    "- ❌ **Digit-word: \"123-neutral\"** → Filtered out (digit-word pattern)\n",
    "- ✅ **\"25º celsius\"** → `['celsius']` (º treated as punctuation)\n",
    "\n",
    "**Final Result:** Only meaningful, clean tokens ≥ 3 characters from appropriate entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5b584",
   "metadata": {},
   "source": [
    "# Draft tests (unitary tests TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification test for the fixes\n",
    "print(\"=\"*50)\n",
    "print(\"VERIFYING FIXES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test the corrected time pattern function\n",
    "test_time_tokens = [\"3PM\", \"10AM\", \"5PA\", \"12AL\"]\n",
    "print(\"Testing corrected time patterns:\")\n",
    "for token in test_time_tokens:\n",
    "    matches = matches_time_pattern(token)\n",
    "    print(f\"'{token}' -> matches time pattern: {matches}\")\n",
    "\n",
    "# Test º character removal\n",
    "test_text = \"Temperature: 25º celsius\"\n",
    "print(f\"\\nTesting º removal:\")\n",
    "print(f\"Original: '{test_text}'\")\n",
    "tokens = tokenize_text(test_text)\n",
    "print(f\"Tokens: {sorted(tokens)}\")\n",
    "\n",
    "# Test all filtering combined\n",
    "test_combined_text = \"Meeting at 3PM, temperature 25º, status: 123-neutral, numbers 456\"\n",
    "print(f\"\\nTesting combined filtering:\")\n",
    "print(f\"Original: '{test_combined_text}'\")\n",
    "tokens_combined = tokenize_text(test_combined_text)\n",
    "print(f\"Final tokens: {sorted(tokens_combined)}\")\n",
    "\n",
    "print(\"\\nExpected results:\")\n",
    "print(\"- Time patterns (3PM) should be filtered out\")\n",
    "print(\"- Numbers (456) should be filtered out\") \n",
    "print(\"- Digit-word patterns (123-neutral) should be filtered out\")\n",
    "print(\"- º should be treated as punctuation\")\n",
    "print(\"- Only meaningful words should remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2967cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fix for HTML br and p tag handling\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING HTML BR AND P TAG HANDLING FIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test cases that demonstrate the issue and fix\n",
    "test_html_cases = [\n",
    "    \"Ankama&lt;br&gt;&lt;br&gt;1.\",\n",
    "    \"Word1&lt;br&gt;Word2\",\n",
    "    \"Start&lt;p&gt;Middle&lt;/p&gt;End\",\n",
    "    \"Text&lt;br/&gt;More text\",\n",
    "    \"Line1&lt;BR&gt;Line2\",  # Test case insensitive\n",
    "    \"Para&lt;P class='test'&gt;Content&lt;/P&gt;After\",\n",
    "    \"Normal text without HTML tags\"\n",
    "]\n",
    "\n",
    "print(\"Testing HTML tag removal with br/p handling:\")\n",
    "for text in test_html_cases:\n",
    "    cleaned = remove_html_tags(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    print(f\"Original: '{text}'\")\n",
    "    print(f\"Cleaned:  '{cleaned}'\")\n",
    "    print(f\"Tokens:   {sorted(tokens)}\")\n",
    "    print()\n",
    "\n",
    "# Specific test for the reported issue\n",
    "print(\"=\"*40)\n",
    "print(\"SPECIFIC TEST FOR REPORTED ISSUE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "issue_text = \"Ankama&lt;br&gt;&lt;br&gt;1.\"\n",
    "print(f\"Testing: '{issue_text}'\")\n",
    "\n",
    "# Before fix (simulate): would result in \"Ankama1\"\n",
    "# After fix: should result in separate tokens\n",
    "cleaned_text = remove_html_tags(issue_text)\n",
    "final_tokens = tokenize_text(issue_text)\n",
    "\n",
    "print(f\"HTML removed: '{cleaned_text}'\")\n",
    "print(f\"Final tokens: {sorted(final_tokens)}\")\n",
    "print(f\"✅ Issue fixed: 'Ankama' and other meaningful tokens are separate\" if 'Ankama' in final_tokens else \"❌ Issue not fixed\")\n",
    "\n",
    "# Test with a more complex example\n",
    "complex_html = \"Company&lt;br&gt;&lt;br&gt;Address&lt;p&gt;City&lt;/p&gt;Country123\"\n",
    "print(f\"\\nComplex example: '{complex_html}'\")\n",
    "complex_tokens = tokenize_text(complex_html)\n",
    "print(f\"Tokens: {sorted(complex_tokens)}\")\n",
    "print(\"Expected: Company, Address, City, Country123 should be separate tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new ignore_identical_translation parameter\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING ignore_identical_translation PARAMETER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test data with identical translations\n",
    "test_data_identical = {\n",
    "    'key': ['greeting', 'same1', 'same2', 'different'],\n",
    "    'fr-fr': ['Bonjour', 'Same Text', 'Identical', 'Source Text'],\n",
    "    'es-es': ['Hola', 'Same Text', 'Identical', 'Target Text']  # First two are identical to source\n",
    "}\n",
    "\n",
    "df_identical = pd.DataFrame(test_data_identical)\n",
    "df_identical.to_excel(\"test_identical.xlsx\", index=False)\n",
    "print(\"Test Excel file with identical translations created!\")\n",
    "print(\"Test data:\")\n",
    "print(df_identical.to_string(index=False))\n",
    "\n",
    "# Test with ignore_identical_translation=True (default)\n",
    "print(f\"\\n1. Testing with ignore_identical_translation=True (default):\")\n",
    "try:\n",
    "    tokens_ignore_true = process_file(\"test_identical.xlsx\", \"es-es\", \"tokens_ignore_true.txt\")\n",
    "    print(f\"Tokens with ignore=True: {sorted(tokens_ignore_true)}\")\n",
    "    print(\"Expected: 'Same Text' and 'Identical' should be skipped\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with ignore_identical_translation=False\n",
    "print(f\"\\n2. Testing with ignore_identical_translation=False:\")\n",
    "try:\n",
    "    tokens_ignore_false = process_file(\"test_identical.xlsx\", \"es-es\", \"tokens_ignore_false.txt\", ignore_identical_translation=False)\n",
    "    print(f\"Tokens with ignore=False: {sorted(tokens_ignore_false)}\")\n",
    "    print(\"Expected: 'Same Text' and 'Identical' should be included\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Show the difference\n",
    "if 'tokens_ignore_true' in locals() and 'tokens_ignore_false' in locals():\n",
    "    additional_tokens = tokens_ignore_false - tokens_ignore_true\n",
    "    print(f\"\\nAdditional tokens when ignore_identical_translation=False: {sorted(additional_tokens)}\")\n",
    "\n",
    "# Also test with XLIFF\n",
    "test_xliff_identical = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<xliff version=\"1.2\" xmlns=\"urn:oasis:names:tc:xliff:document:1.2\">\n",
    "    <file datatype=\"plaintext\" original=\"test\" source-language=\"fr-fr\" target-language=\"es-es\">\n",
    "        <body>\n",
    "            <trans-unit id=\"test.1\">\n",
    "                <source>Hello World</source>\n",
    "                <target>Hola Mundo</target>\n",
    "            </trans-unit>\n",
    "            <trans-unit id=\"test.2\">\n",
    "                <source>Same Text</source>\n",
    "                <target>Same Text</target>\n",
    "            </trans-unit>\n",
    "            <trans-unit id=\"test.3\">\n",
    "                <source>Identical</source>\n",
    "                <target>Identical</target>\n",
    "            </trans-unit>\n",
    "        </body>\n",
    "    </file>\n",
    "</xliff>\"\"\"\n",
    "\n",
    "with open(\"test_identical.xliff\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_xliff_identical)\n",
    "\n",
    "print(f\"\\n3. Testing XLIFF with ignore_identical_translation=True:\")\n",
    "try:\n",
    "    xliff_tokens_true = process_file(\"test_identical.xliff\", \"es-es\", \"xliff_tokens_true.txt\")\n",
    "    print(f\"XLIFF tokens with ignore=True: {sorted(xliff_tokens_true)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\n4. Testing XLIFF with ignore_identical_translation=False:\")\n",
    "try:\n",
    "    xliff_tokens_false = process_file(\"test_identical.xliff\", \"es-es\", \"xliff_tokens_false.txt\", ignore_identical_translation=False)\n",
    "    print(f\"XLIFF tokens with ignore=False: {sorted(xliff_tokens_false)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Clean up test files\n",
    "print(\"\\nCleaning up test files...\")\n",
    "test_files = [\n",
    "    \"test_identical.xlsx\", \"test_identical.xliff\",\n",
    "    \"tokens_ignore_true.txt\", \"tokens_ignore_false.txt\",\n",
    "    \"xliff_tokens_true.txt\", \"xliff_tokens_false.txt\"\n",
    "]\n",
    "for file in test_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Removed: {file}\")\n",
    "\n",
    "print(\"\\nParameter test completed!\")\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"- ignore_identical_translation=True (default): Skips entries where target equals source\")\n",
    "print(\"- ignore_identical_translation=False: Includes all entries, even identical translations\")\n",
    "print(\"- This allows users to control whether to include identical translations in their token extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea1e4a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb564bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import product\n",
    "\n",
    "def demorph_string(input_string):\n",
    "    \"\"\"\n",
    "    Expand morphological patterns in localization strings.\n",
    "    \n",
    "    Supports two pattern types:\n",
    "    1. Tilde patterns: {~X...} where X is a letter and ... is suffix\n",
    "    2. Square bracket patterns: {[N*]?option1:option2} where N is a digit\n",
    "    \n",
    "    Args:\n",
    "        input_string (str): String containing morphological patterns\n",
    "        \n",
    "    Returns:\n",
    "        str: String with all variations joined by spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_tilde_patterns(text):\n",
    "        \"\"\"Extract all tilde morphological patterns from a word.\"\"\"\n",
    "        pattern_regex = r'\\{~([^}]+)\\}'\n",
    "        matches = re.findall(pattern_regex, text)\n",
    "        parsed_patterns = []\n",
    "        for match in matches:\n",
    "            # Split by ~ to handle multiple patterns in the same braces\n",
    "            sub_patterns = match.split('~')\n",
    "            for sub_pattern in sub_patterns:\n",
    "                if len(sub_pattern) >= 1:\n",
    "                    letter = sub_pattern[0]\n",
    "                    suffix = sub_pattern[1:] if len(sub_pattern) > 1 else \"\"\n",
    "                    parsed_patterns.append((letter, suffix))\n",
    "        return parsed_patterns\n",
    "    \n",
    "    def extract_bracket_patterns(text):\n",
    "        \"\"\"Extract all bracket patterns from a word.\"\"\"\n",
    "        # Pattern: {[digit*]?option1:option2} or {[~digit]?option1:option2}\n",
    "        pattern_regex = r'\\{\\[([~]?\\d+\\*?)\\]\\?([^:}]*):([^}]*)\\}'\n",
    "        matches = re.findall(pattern_regex, text)\n",
    "        return matches\n",
    "    \n",
    "    def generate_tilde_variations(base_word, patterns):\n",
    "        \"\"\"Generate variations for tilde patterns.\"\"\"\n",
    "        # Remove patterns from base word to get the root\n",
    "        root = re.sub(r'\\{~[^}]+\\}', '', base_word)\n",
    "        \n",
    "        # Check if root should be excluded (if 's' or 'm' patterns present)\n",
    "        pattern_letters = [p[0] for p in patterns]\n",
    "        exclude_root = 's' in pattern_letters or 'm' in pattern_letters\n",
    "        \n",
    "        # If no patterns, return the original word\n",
    "        if not patterns:\n",
    "            return [base_word]\n",
    "        \n",
    "        variations = []\n",
    "        \n",
    "        # Group patterns by type\n",
    "        gender_patterns = [(letter, suffix) for letter, suffix in patterns if letter in 'mf']\n",
    "        number_patterns = [(letter, suffix) for letter, suffix in patterns if letter in 'sp']\n",
    "        \n",
    "        # Handle gender+number combinations\n",
    "        if gender_patterns and number_patterns:\n",
    "            # We need all 4 combinations: masc sing, fem sing, masc plural, fem plural\n",
    "            \n",
    "            # 1. Masculine singular (root) - only if not excluded\n",
    "            if not exclude_root:\n",
    "                variations.append(root)\n",
    "\n",
    "            # 2. Masculine singular with masculine suffix\n",
    "            for g_letter, g_suffix in gender_patterns:\n",
    "                if g_letter == 'm':\n",
    "                    male_root = root + g_suffix\n",
    "                    variations.append(male_root)\n",
    "\n",
    "            # 3. Feminine singular (root + feminine suffix)\n",
    "            for g_letter, g_suffix in gender_patterns:\n",
    "                if g_letter == 'f':\n",
    "                    variations.append(root + g_suffix)\n",
    "            \n",
    "            # 4. Masculine plural (root + plural suffix)  \n",
    "            for n_letter, n_suffix in number_patterns:\n",
    "                if n_letter == 'p':\n",
    "                    variations.append(root + n_suffix)\n",
    "            \n",
    "            # 5. Feminine plural (root + feminine suffix + plural suffix)\n",
    "            for (g_letter, g_suffix), (n_letter, n_suffix) in product(gender_patterns, number_patterns):\n",
    "                if g_letter == 'f' and n_letter == 'p':\n",
    "                    variations.append(root + g_suffix + n_suffix)\n",
    "                    \n",
    "        else:\n",
    "            # Handle simple cases (no combinations needed)\n",
    "            \n",
    "            # If root should be included, add it first\n",
    "            if not exclude_root:\n",
    "                variations.append(root)\n",
    "            \n",
    "            # Add individual pattern variations\n",
    "            for letter, suffix in patterns:\n",
    "                variation = root + suffix\n",
    "                variations.append(variation)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_variations = []\n",
    "        for var in variations:\n",
    "            if var not in seen:\n",
    "                seen.add(var)\n",
    "                unique_variations.append(var)\n",
    "        \n",
    "        return unique_variations\n",
    "    \n",
    "    def generate_bracket_variations(base_word, bracket_patterns):\n",
    "        \"\"\"Generate variations for bracket patterns.\"\"\"\n",
    "        if not bracket_patterns:\n",
    "            return [base_word]\n",
    "        \n",
    "        current_variations = [base_word]\n",
    "        \n",
    "        for pattern_match, option1, option2 in bracket_patterns:\n",
    "            new_variations = []\n",
    "            \n",
    "            # Build the regex pattern correctly\n",
    "            pattern_to_replace = r'\\{\\['  # {[\n",
    "            pattern_to_replace += re.escape(pattern_match)  # pattern (escaped)\n",
    "            pattern_to_replace += r'\\]\\?'  # ]?\n",
    "            pattern_to_replace += re.escape(option1)  # option1 (escaped)\n",
    "            pattern_to_replace += ':'  # :\n",
    "            pattern_to_replace += re.escape(option2)  # option2 (escaped)\n",
    "            pattern_to_replace += r'\\}'  # }\n",
    "            \n",
    "            for current_var in current_variations:\n",
    "                # For the pattern {[N*]?option1:option2}:\n",
    "                # Generate variation 1: condition true -> use option1 (usually the base/unmarked form)\n",
    "                var1 = re.sub(pattern_to_replace, option1, current_var, count=1)\n",
    "                if var1 not in new_variations:\n",
    "                    new_variations.append(var1)\n",
    "                \n",
    "                # Generate variation 2: condition false -> use option2 (usually the marked form)\n",
    "                var2 = re.sub(pattern_to_replace, option2, current_var, count=1)\n",
    "                if var2 not in new_variations:\n",
    "                    new_variations.append(var2)\n",
    "            \n",
    "            current_variations = new_variations\n",
    "        \n",
    "        return current_variations\n",
    "\n",
    "    # Find all words with patterns (both types)\n",
    "    word_pattern_regex = r'\\S*\\{[~\\[][^}]+\\}(?:\\{[~\\[][^}]+\\})*'\n",
    "    \n",
    "    def replace_word_patterns(match):\n",
    "        word_with_patterns = match.group(0)\n",
    "        \n",
    "        # Check what type of patterns we have\n",
    "        bracket_patterns = extract_bracket_patterns(word_with_patterns)\n",
    "        tilde_patterns = extract_tilde_patterns(word_with_patterns)\n",
    "        \n",
    "        if bracket_patterns and not tilde_patterns:\n",
    "            # Only bracket patterns\n",
    "            variations = generate_bracket_variations(word_with_patterns, bracket_patterns)\n",
    "        elif tilde_patterns and not bracket_patterns:\n",
    "            # Only tilde patterns\n",
    "            variations = generate_tilde_variations(word_with_patterns, tilde_patterns)\n",
    "        elif bracket_patterns and tilde_patterns:\n",
    "            # Both types - handle bracket first, then tilde\n",
    "            bracket_variations = generate_bracket_variations(word_with_patterns, bracket_patterns)\n",
    "            final_variations = []\n",
    "            for var in bracket_variations:\n",
    "                if extract_tilde_patterns(var):\n",
    "                    tilde_vars = generate_tilde_variations(var, extract_tilde_patterns(var))\n",
    "                    final_variations.extend(tilde_vars)\n",
    "                else:\n",
    "                    final_variations.append(var)\n",
    "            variations = final_variations\n",
    "        else:\n",
    "            # No patterns found (shouldn't happen with our regex)\n",
    "            variations = [word_with_patterns]\n",
    "        \n",
    "        return ' '.join(variations)\n",
    "    \n",
    "    # Replace all pattern words with their variations\n",
    "    result = re.sub(word_pattern_regex, replace_word_patterns, input_string)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4292b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing demorph function with ALL test cases:\n",
      "============================================================\n",
      "Input:    Apariencia{[~1]?s:} de montura\n",
      "Expected: Apariencia Apariencias de montura\n",
      "Result:   Apariencias Apariencia de montura\n",
      "Match:    Same words (different order) ✅\n",
      "----------------------------------------\n",
      "Input:    Transmutaci{[~1]?ones:ón}\n",
      "Expected: Transmutación Transmutaciones\n",
      "Result:   Transmutaciones Transmutación\n",
      "Match:    Same words (different order) ✅\n",
      "----------------------------------------\n",
      "Input:    Fragmento{[~1]?s:} de Relíquia{[~1]?s:}\n",
      "Expected: Fragmentos Fragmento de Relíquias Relíquia\n",
      "Result:   Fragmentos Fragmento de Relíquias Relíquia\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Display Window{[~1]?s:} & Workshop{[~1]?s:}\n",
      "Expected: Display Windows Window & Workshops Workshop\n",
      "Result:   Display Windows Window & Workshops Workshop\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Costume d'ouvri{[1*]?ère:er} de l'usine\n",
      "Expected: Costume d'ouvrier d'ouvrière de l'usine\n",
      "Result:   Costume d'ouvrière d'ouvrier de l'usine\n",
      "Match:    Same words (different order) ✅\n",
      "----------------------------------------\n",
      "Input:    Título: Campeã{[1*]?:o} do Torneio de Verão\n",
      "Expected: Título: Campeã Campeão do Torneio de Verão\n",
      "Result:   Título: Campeã Campeão do Torneio de Verão\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Titre : Dragonisat{[1*]?rice:eur} Ultime\n",
      "Expected: Titre : Dragonisatrice Dragonisateur Ultime\n",
      "Result:   Titre : Dragonisatrice Dragonisateur Ultime\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Title: Ultimate Dragonizer{[3*]?:}\n",
      "Expected: Title: Ultimate Dragonizer\n",
      "Result:   Title: Ultimate Dragonizer\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Título: Dragonizador{[2*]?a:} definitivo\n",
      "Expected: Título: Dragonizadora Dragonizador definitivo\n",
      "Result:   Título: Dragonizadora Dragonizador definitivo\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Título: {[1*]?Dragonizadora Suprema:Dragonizador Supremo}\n",
      "Expected: Título: Dragonizadora Suprema Dragonizador Supremo\n",
      "Result:   Título: Dragonizadora Suprema Dragonizador Supremo\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Misi{~són~pones}\n",
      "Expected: Misión Misiones\n",
      "Result:   Misión Misiones\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    %1 posede %2 personaje{~ps} en este servidor\n",
      "Expected: %1 posede %2 personaje personajes en este servidor\n",
      "Result:   %1 posede %2 personaje personajes en este servidor\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Possedé{~fe}{~ps}\n",
      "Expected: Possedé Possedée Possedés Possedées\n",
      "Result:   Possedé Possedée Possedés Possedées\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    %1 misi{~són}{~pones} pendiente{~ps}\n",
      "Expected: %1 misión misiones pendiente pendientes\n",
      "Result:   %1 misión misiones pendiente pendientes\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Espos{~mo}{~fa}\n",
      "Expected: Esposo Esposa\n",
      "Result:   Esposo Esposa\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Jugador{[3*]?a:} premium\n",
      "Expected: Jugador Jugadora premium\n",
      "Result:   Jugadora Jugador premium\n",
      "Match:    Same words (different order) ✅\n",
      "----------------------------------------\n",
      "Input:    Vendedor{[42*]?a:} oficial\n",
      "Expected: Vendedora Vendedor oficial\n",
      "Result:   Vendedora Vendedor oficial\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "Input:    Administrador{[999*]?a:} del sistema\n",
      "Expected: Administradora Administrador del sistema\n",
      "Result:   Administradora Administrador del sistema\n",
      "Match:    Exact ✅\n",
      "----------------------------------------\n",
      "\n",
      "Summary: 18/18 tests passed (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Test all cases from the test suite with clear input/output display\n",
    "test_cases = [\n",
    "    # test_basic_suffix_patterns\n",
    "    (\"Apariencia{[~1]?s:} de montura\", \"Apariencia Apariencias de montura\"),\n",
    "    (\"Transmutaci{[~1]?ones:ón}\", \"Transmutación Transmutaciones\"),\n",
    "    (\"Fragmento{[~1]?s:} de Relíquia{[~1]?s:}\", \"Fragmentos Fragmento de Relíquias Relíquia\"),\n",
    "    \n",
    "    # test_english_plurals\n",
    "    (\"Display Window{[~1]?s:} & Workshop{[~1]?s:}\", \"Display Windows Window & Workshops Workshop\"),\n",
    "    \n",
    "    # test_gender_patterns\n",
    "    (\"Costume d'ouvri{[1*]?ère:er} de l'usine\", \"Costume d'ouvrier d'ouvrière de l'usine\"),\n",
    "    (\"Título: Campeã{[1*]?:o} do Torneio de Verão\", \"Título: Campeã Campeão do Torneio de Verão\"),\n",
    "    (\"Titre : Dragonisat{[1*]?rice:eur} Ultime\", \"Titre : Dragonisatrice Dragonisateur Ultime\"),\n",
    "    \n",
    "    # test_other_digits\n",
    "    (\"Title: Ultimate Dragonizer{[3*]?:}\", \"Title: Ultimate Dragonizer\"),\n",
    "    (\"Título: Dragonizador{[2*]?a:} definitivo\", \"Título: Dragonizadora Dragonizador definitivo\"),\n",
    "    \n",
    "    # test_standalone_pattern\n",
    "    (\"Título: {[1*]?Dragonizadora Suprema:Dragonizador Supremo}\", \"Título: Dragonizadora Suprema Dragonizador Supremo\"),\n",
    "    \n",
    "    # test_tilde_patterns (key cases with grammar codes)\n",
    "    (\"Misi{~són~pones}\", \"Misión Misiones\"),\n",
    "    \n",
    "    # test_additional_cases\n",
    "    (\"%1 posede %2 personaje{~ps} en este servidor\", \"%1 posede %2 personaje personajes en este servidor\"),\n",
    "    (\"Possedé{~fe}{~ps}\", \"Possedé Possedée Possedés Possedées\"),\n",
    "    (\"%1 misi{~són}{~pones} pendiente{~ps}\", \"%1 misión misiones pendiente pendientes\"),\n",
    "    (\"Espos{~mo}{~fa}\", \"Esposo Esposa\"),\n",
    "    \n",
    "    # test_any_digit_patterns\n",
    "    (\"Jugador{[3*]?a:} premium\", \"Jugador Jugadora premium\"),\n",
    "    (\"Vendedor{[42*]?a:} oficial\", \"Vendedora Vendedor oficial\"),\n",
    "    (\"Administrador{[999*]?a:} del sistema\", \"Administradora Administrador del sistema\"),\n",
    "]\n",
    "\n",
    "# Test the demorph function with all test cases\n",
    "# Modified to check if result and expected have same set of words regardless of order\n",
    "print(\"Testing demorph function with ALL test cases:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def words_match(result, expected):\n",
    "    \"\"\"Check if two strings have the same set of unique words regardless of order.\"\"\"\n",
    "    result_words = set(result.split())\n",
    "    expected_words = set(expected.split())\n",
    "    return result_words == expected_words\n",
    "\n",
    "passed = 0\n",
    "total = 0\n",
    "\n",
    "for input_str, expected in test_cases:\n",
    "    result = demorph_string(input_str)\n",
    "    \n",
    "    # Check both exact match and word set match\n",
    "    exact_match = result == expected\n",
    "    words_same = words_match(result, expected)\n",
    "    \n",
    "    total += 1\n",
    "    if words_same:\n",
    "        passed += 1\n",
    "    \n",
    "    print(f\"Input:    {input_str}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Result:   {result}\")\n",
    "    \n",
    "    # Show different types of matches\n",
    "    if exact_match:\n",
    "        print(f\"Match:    Exact ✅\")\n",
    "    elif words_same:\n",
    "        print(f\"Match:    Same words (different order) ✅\")\n",
    "    else:\n",
    "        print(f\"Match:    Failed ❌\")\n",
    "        # Show word difference for debugging\n",
    "        expected_words = set(expected.split())\n",
    "        result_words = set(result.split())\n",
    "        if expected_words != result_words:\n",
    "            missing = expected_words - result_words\n",
    "            extra = result_words - expected_words\n",
    "            if missing:\n",
    "                print(f\"          Missing words: {missing}\")\n",
    "            if extra:\n",
    "                print(f\"          Extra words: {extra}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nSummary: {passed}/{total} tests passed ({passed/total*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
