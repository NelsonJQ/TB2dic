{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc357496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete working example with fixed functions\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Set, List\n",
    "\n",
    "def detect_file_type(file_path: str) -> str:\n",
    "    \"\"\"Detect if file is Excel or XLIFF based on extension\"\"\"\n",
    "    extension = Path(file_path).suffix.lower()\n",
    "    if extension in ['.xlsx', '.xls']:\n",
    "        return 'excel'\n",
    "    elif extension in ['.xliff', '.xlf', '.xml']:\n",
    "        return 'xliff'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {extension}\")\n",
    "\n",
    "def tokenize_text(text: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text by whitespace and punctuation, keeping hyphens and apostrophes\n",
    "    Returns unique tokens without case normalization\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return set()\n",
    "    \n",
    "    # Remove HTML tags and decode entities first\n",
    "    cleaned_text = remove_html_tags(text)\n",
    "    \n",
    "    # Remove hyperlinks and email addresses\n",
    "    cleaned_text = remove_hyperlinks_and_emails(cleaned_text)\n",
    "    \n",
    "    # Replace punctuation except hyphens (-) and apostrophes (') with spaces\n",
    "    # Include º in the punctuation list to be replaced with spaces\n",
    "    cleaned_text = re.sub(r'[^\\w\\s\\'-]|º', ' ', cleaned_text)\n",
    "    \n",
    "    # Split by whitespace and filter out empty strings\n",
    "    tokens = [token for token in cleaned_text.split() if token]\n",
    "    \n",
    "    # Clean token edges (remove leading/trailing apostrophes and hyphens)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        cleaned_token = clean_token_edges(token)\n",
    "        if cleaned_token:  # Only add non-empty tokens\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "    \n",
    "    # Apply additional filtering\n",
    "    token_set = set(cleaned_tokens)\n",
    "    filtered_tokens = filter_tokens(token_set)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def has_square_brackets(text: str) -> bool:\n",
    "    \"\"\"Check if text contains content inside square brackets [...]\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return False\n",
    "    return bool(re.search(r'\\[.+\\]', str(text)))\n",
    "\n",
    "def remove_hyperlinks_and_emails(text: str) -> str:\n",
    "    \"\"\"Remove hyperlinks and email addresses from text\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs (http, https, ftp, www)\n",
    "    text = re.sub(r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+', '', text)\n",
    "    text = re.sub(r'ftp://[^\\s<>\"{}|\\\\^`\\[\\]]+', '', text)\n",
    "    text = re.sub(r'www\\.[^\\s<>\"{}|\\\\^`\\[\\]]+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags and decode HTML entities from text\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    import html\n",
    "    text = str(text)\n",
    "    \n",
    "    # Decode HTML entities first (e.g., &lt; -> <, &quot; -> \")\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Replace <br> and <p> tags with whitespace to prevent word concatenation\n",
    "    # Handle both opening and closing tags, and self-closing tags\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'</?p\\b[^>]*>', ' ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove all other HTML tags using regex\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_all_caps_string(text: str) -> bool:\n",
    "    \"\"\"Check if the entire string is in all caps (excluding spaces and punctuation)\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    # Extract only alphabetic characters\n",
    "    letters_only = ''.join(c for c in str(text) if c.isalpha())\n",
    "    \n",
    "    # If no letters, return False; otherwise check if all are uppercase\n",
    "    return len(letters_only) > 0 and letters_only.isupper()\n",
    "\n",
    "def is_just_numbers(token: str) -> bool:\n",
    "    \"\"\"Check if token consists only of digits\"\"\"\n",
    "    if not token:\n",
    "        return False\n",
    "    return token.isdigit()\n",
    "\n",
    "def matches_time_pattern(token: str) -> bool:\n",
    "    \"\"\"Check if token matches pattern: digit(s) followed by PA, PM, or AL\"\"\"\n",
    "    if not token:\n",
    "        return False\n",
    "    return bool(re.match(r'^\\d+(PA|PM|AM|AL)$', token, re.IGNORECASE))\n",
    "\n",
    "def matches_digit_word_pattern(token: str) -> bool:\n",
    "    \"\"\"Check if token matches pattern: digit(s)-word (e.g., 123-neutral)\"\"\"\n",
    "    if not token:\n",
    "        return False\n",
    "    return bool(re.match(r'^\\d+-\\w+$', token))\n",
    "\n",
    "def clean_token_edges(token: str) -> str:\n",
    "    \"\"\"Remove apostrophes and hyphens from the start and end of tokens\"\"\"\n",
    "    if not token:\n",
    "        return token\n",
    "    \n",
    "    # Remove apostrophes and hyphens from start and end repeatedly\n",
    "    while token and token[0] in \"'-\":\n",
    "        token = token[1:]\n",
    "    while token and token[-1] in \"'-\":\n",
    "        token = token[:-1]\n",
    "    \n",
    "    return token\n",
    "\n",
    "def is_same_character_chain(token: str) -> bool:\n",
    "    \"\"\"Check if token is a chain of the same character (case-insensitive)\"\"\"\n",
    "    if not token or len(token) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Convert to lowercase and check if all characters are the same\n",
    "    lower_token = token.lower()\n",
    "    return len(set(lower_token)) == 1\n",
    "\n",
    "def filter_tokens(tokens: Set[str]) -> Set[str]:\n",
    "    \"\"\"Apply additional filtering to tokens\"\"\"\n",
    "    filtered_tokens = set()\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Remove tokens with length < 3\n",
    "        if len(token) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Remove tokens that are chains of the same character\n",
    "        if is_same_character_chain(token):\n",
    "            continue\n",
    "        \n",
    "        # Remove tokens that are just numbers\n",
    "        if is_just_numbers(token):\n",
    "            continue\n",
    "            \n",
    "        # Remove tokens matching time patterns (digit(s) + PA/PM/AL)\n",
    "        if matches_time_pattern(token):\n",
    "            continue\n",
    "            \n",
    "        # Remove tokens matching digit-word patterns (e.g., 123-neutral)\n",
    "        if matches_digit_word_pattern(token):\n",
    "            continue\n",
    "            \n",
    "        filtered_tokens.add(token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def process_excel_file(file_path: str, language_code: str, ignore_identical_translation: bool = True) -> Set[str]:\n",
    "    \"\"\"Process Excel file and extract tokens from the specified language column\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        if language_code not in df.columns:\n",
    "            raise ValueError(f\"Language code '{language_code}' not found in Excel columns: {list(df.columns)}\")\n",
    "        \n",
    "        all_tokens = set()\n",
    "        \n",
    "        # Try to find the source language column (typically fr-fr)\n",
    "        source_column = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() == 'fr-fr':\n",
    "                source_column = col\n",
    "                break\n",
    "        \n",
    "        # Process all values in the language column\n",
    "        for idx, value in enumerate(df[language_code]):\n",
    "            if pd.notna(value):\n",
    "                # Check if corresponding source contains square brackets\n",
    "                skip_entry = False\n",
    "                source_value = None\n",
    "                \n",
    "                if source_column and idx < len(df[source_column]):\n",
    "                    source_value = df[source_column].iloc[idx]\n",
    "                    if has_square_brackets(source_value):\n",
    "                        skip_entry = True\n",
    "                        print(f\"Skipping entry with square brackets in source: {source_value}\")\n",
    "                \n",
    "                # Check if target equals source\n",
    "                if not skip_entry and ignore_identical_translation and source_value is not None:\n",
    "                    if str(value).strip() == str(source_value).strip():\n",
    "                        skip_entry = True\n",
    "                        print(f\"Skipping entry where target equals source: {value}\")\n",
    "                \n",
    "                # Check if target text is all caps\n",
    "                if not skip_entry and is_all_caps_string(str(value)):\n",
    "                    skip_entry = True\n",
    "                    print(f\"Skipping entry with all-caps target: {value}\")\n",
    "                \n",
    "                if not skip_entry:\n",
    "                    tokens = tokenize_text(str(value))\n",
    "                    all_tokens.update(tokens)\n",
    "        \n",
    "        return all_tokens\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing Excel file: {str(e)}\")\n",
    "\n",
    "def process_xliff_file(file_path: str, language_code: str, ignore_identical_translation: bool = True) -> Set[str]:\n",
    "    \"\"\"Process XLIFF file and extract tokens from source or target elements\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        all_tokens = set()\n",
    "        \n",
    "        # Find all file elements (handle both with and without namespace)\n",
    "        file_elements = []\n",
    "        for elem in root.iter():\n",
    "            if elem.tag.endswith('}file') or elem.tag == 'file':\n",
    "                file_elements.append(elem)\n",
    "        \n",
    "        for file_elem in file_elements:\n",
    "            source_lang = file_elem.get('source-language', '').lower()\n",
    "            target_lang = file_elem.get('target-language', '').lower()\n",
    "            \n",
    "            # Determine which elements to extract based on language code\n",
    "            extract_source = source_lang == language_code.lower()\n",
    "            extract_target = target_lang == language_code.lower()\n",
    "            \n",
    "            if not (extract_source or extract_target):\n",
    "                continue\n",
    "            \n",
    "            # Find all trans-unit elements\n",
    "            trans_units = []\n",
    "            for elem in file_elem.iter():\n",
    "                if elem.tag.endswith('}trans-unit') or elem.tag == 'trans-unit':\n",
    "                    trans_units.append(elem)\n",
    "            \n",
    "            for trans_unit in trans_units:\n",
    "                source_text = None\n",
    "                target_text = None\n",
    "                \n",
    "                # First pass: collect source and target texts\n",
    "                for elem in trans_unit:\n",
    "                    if elem.tag.endswith('}source') or elem.tag == 'source':\n",
    "                        source_text = elem.text\n",
    "                    elif elem.tag.endswith('}target') or elem.tag == 'target':\n",
    "                        target_text = elem.text\n",
    "                \n",
    "                # Check if source contains square brackets\n",
    "                skip_entry = source_text and has_square_brackets(source_text)\n",
    "                if skip_entry:\n",
    "                    print(f\"Skipping entry with square brackets in source: {source_text}\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if target equals source\n",
    "                if not skip_entry and ignore_identical_translation and source_text and target_text:\n",
    "                    if source_text.strip() == target_text.strip():\n",
    "                        skip_entry = True\n",
    "                        print(f\"Skipping entry where target equals source: {target_text}\")\n",
    "                        continue\n",
    "                \n",
    "                # Check if target text is all caps (only if we're extracting target)\n",
    "                if not skip_entry and extract_target and target_text:\n",
    "                    if is_all_caps_string(target_text):\n",
    "                        skip_entry = True\n",
    "                        print(f\"Skipping entry with all-caps target: {target_text}\")\n",
    "                        continue\n",
    "                \n",
    "                # Process the appropriate text based on language\n",
    "                if extract_source and source_text:\n",
    "                    tokens = tokenize_text(source_text)\n",
    "                    all_tokens.update(tokens)\n",
    "                \n",
    "                if extract_target and target_text:\n",
    "                    tokens = tokenize_text(target_text)\n",
    "                    all_tokens.update(tokens)\n",
    "        \n",
    "        return all_tokens\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing XLIFF file: {str(e)}\")\n",
    "\n",
    "def export_tokens_to_txt(tokens: Set[str], output_path: str):\n",
    "    \"\"\"Export tokens to a text file, one token per line\"\"\"\n",
    "    sorted_tokens = sorted(tokens)  # Sort for consistent output\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for token in sorted_tokens:\n",
    "            f.write(token + '\\n')\n",
    "    \n",
    "    print(f\"Exported {len(tokens)} unique tokens to: {output_path}\")\n",
    "\n",
    "def process_file(file_path: str, language_code: str, output_path: str = None, ignore_identical_translation: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to process a file and extract tokens for a given language code\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Excel or XLIFF file\n",
    "        language_code: Language code (e.g., \"es-es\")\n",
    "        output_path: Optional output path for the txt file\n",
    "        ignore_identical_translation: If True (default), skip entries where target equals source\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Detect file type\n",
    "    file_type = detect_file_type(file_path)\n",
    "    print(f\"Detected file type: {file_type}\")\n",
    "    \n",
    "    # Process file based on type\n",
    "    if file_type == 'excel':\n",
    "        tokens = process_excel_file(file_path, language_code, ignore_identical_translation)\n",
    "    elif file_type == 'xliff':\n",
    "        tokens = process_xliff_file(file_path, language_code, ignore_identical_translation)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    \n",
    "    print(f\"Found {len(tokens)} unique tokens for language: {language_code}\")\n",
    "    \n",
    "    # Generate output path if not provided\n",
    "    if output_path is None:\n",
    "        base_name = Path(file_path).stem\n",
    "        output_path = f\"{base_name}_{language_code}_tokens.txt\"\n",
    "    \n",
    "    # Export tokens\n",
    "    export_tokens_to_txt(tokens, output_path)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Working demonstration\n",
    "sample_xliff_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<xliff version=\"1.2\" xmlns=\"urn:oasis:names:tc:xliff:document:1.2\">\n",
    "    <file datatype=\"plaintext\" original=\"sample\" source-language=\"fr-fr\" target-language=\"es-es\">\n",
    "        <body>\n",
    "            <trans-unit id=\"test.1\" datatype=\"html\">\n",
    "                <source>Votre alignement est au sommet et vos ennemis n'existent (probablement) plus.</source>\n",
    "                <target>Tu alineamiento está en la cumbre y (probablemente) tus enemigos ya no existen.</target>\n",
    "            </trans-unit>\n",
    "            <trans-unit id=\"test.2\" datatype=\"html\">\n",
    "                <source>Apogée</source>\n",
    "                <target>Apogeo</target>\n",
    "            </trans-unit>\n",
    "        </body>\n",
    "    </file>\n",
    "</xliff>\"\"\"\n",
    "\n",
    "# Create sample file\n",
    "with open(\"sample.xliff\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_xliff_content)\n",
    "\n",
    "print(\"Sample XLIFF file created!\")\n",
    "\n",
    "# Process the sample file for Spanish (es-es)\n",
    "try:\n",
    "    tokens = process_file(\"sample.xliff\", \"es-es\", \"spanish_tokens.txt\")\n",
    "    print(f\"\\nExtracted tokens: {sorted(tokens)}\")\n",
    "    \n",
    "    # Show the content of the output file\n",
    "    with open(\"spanish_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    print(f\"\\nContent of spanish_tokens.txt:\\n{content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Also test with French (fr-fr) to show source extraction\n",
    "try:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing with French (source language):\")\n",
    "    tokens_fr = process_file(\"sample.xliff\", \"fr-fr\", \"french_tokens.txt\")\n",
    "    print(f\"\\nExtracted French tokens: {sorted(tokens_fr)}\")\n",
    "    \n",
    "    # Show the content of the French output file\n",
    "    with open(\"french_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content_fr = f.read()\n",
    "    print(f\"\\nContent of french_tokens.txt:\\n{content_fr}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nDemonstration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0214a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration with Excel file\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with Excel file:\")\n",
    "\n",
    "# Create sample Excel data\n",
    "sample_data = {\n",
    "    'key': ['greeting', 'farewell', 'question'],\n",
    "    'en-us': ['Hello world!', 'Goodbye everyone.', 'How are you?'],\n",
    "    'es-es': ['¡Hola mundo!', 'Adiós a todos.', '¿Cómo estás?'],\n",
    "    'fr-fr': ['Bonjour le monde!', 'Au revoir tout le monde.', 'Comment allez-vous?']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.to_excel(\"sample.xlsx\", index=False)\n",
    "print(\"Sample Excel file created!\")\n",
    "print(f\"Excel columns: {list(df.columns)}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Process Excel file for Spanish\n",
    "try:\n",
    "    print(f\"\\nProcessing Excel for es-es:\")\n",
    "    tokens_excel_es = process_file(\"sample.xlsx\", \"es-es\", \"excel_spanish_tokens.txt\", ignore_identical_translation=False)\n",
    "    print(f\"Extracted tokens: {sorted(tokens_excel_es)}\")\n",
    "    \n",
    "    # Show content\n",
    "    with open(\"excel_spanish_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    print(f\"\\nContent of excel_spanish_tokens.txt:\\n{content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Process Excel file for English\n",
    "try:\n",
    "    print(f\"\\nProcessing Excel for en-us:\")\n",
    "    tokens_excel_en = process_file(\"sample.xlsx\", \"en-us\", \"excel_english_tokens.txt\")\n",
    "    print(f\"Extracted tokens: {sorted(tokens_excel_en)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Clean up all files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cleaning up files...\")\n",
    "files_to_remove = [\n",
    "    \"sample.xliff\", \"sample.xlsx\", \n",
    "    \"spanish_tokens.txt\", \"french_tokens.txt\",\n",
    "    \"excel_spanish_tokens.txt\", \"excel_english_tokens.txt\"\n",
    "]\n",
    "\n",
    "for file in files_to_remove:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Removed: {file}\")\n",
    "\n",
    "print(\"\\nAll demonstrations completed successfully!\")\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"- The script can handle both Excel (.xlsx, .xls) and XLIFF (.xliff, .xlf, .xml) files\")\n",
    "print(\"- For Excel: extracts text from the column matching the language code\")\n",
    "print(\"- For XLIFF: extracts text from <source> or <target> elements based on source-language/target-language attributes\")\n",
    "print(\"- Tokenizes by whitespace and punctuation, preserving hyphens (-) and apostrophes (')\")\n",
    "print(\"- Exports unique tokens (case-sensitive) to a text file, one token per line\")\n",
    "print(\"- Usage: process_file(file_path, language_code, optional_output_path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032a6f4",
   "metadata": {},
   "source": [
    "# Final run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFILE_PATH = r\"C:\\Users\\Nelso\\Downloads\\2025-06-13_Retro_TB_as at 6 May 2024.xlsx\" # Excel file path (terminology base)\n",
    "LANGFILE_PATH = r\"C:\\Users\\Nelso\\Downloads\\export.2025-05-06_14-03-35.fr-fr.es-es.xliff\"  # Path to the sample XLIFF file\n",
    "LANG_CODE = \"es-es\"\n",
    "#EXPORT_PATH = r\"C:\\Users\\Nelso\\Downloads\\spanishTB_tokens.txt\"\n",
    "EXPORT_PATH = r\"C:\\Users\\Nelso\\Downloads\\spanish_tokens.txt\"\n",
    "# Process the sample file for Spanish (es-es)\n",
    "try:\n",
    "    tokens = process_file(LANGFILE_PATH, LANG_CODE, EXPORT_PATH, ignore_identical_translation=True)\n",
    "    #print(f\"\\nExtracted tokens: {sorted(tokens)}\")\n",
    "    \n",
    "    # Show the content of the output file\n",
    "    #with open(\"spanish_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "     #   content = f.read()\n",
    "    #print(f\"\\nContent of spanish_tokens.txt:\\n{content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc19924",
   "metadata": {},
   "source": [
    "# Merge both token files\n",
    "\n",
    "Output : single list merged from the TB list + TM list.\n",
    "Purpose: Useful to avoid problematic non-translations in the TM (élément_FR, élément[WIP]_ES), and add the curated non-translation terms from the terminology base (Wabbit_FR = Wabbit_ES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb44d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_PATH1 = r\"C:\\Users\\Nelso\\Downloads\\spanishTB_tokens.txt\" #from TB\n",
    "TXT_PATH2 = r\"C:\\Users\\Nelso\\Downloads\\spanish_tokens.txt\" #from TM\n",
    "# Merge two text files into one with unique tokens\n",
    "def merge_token_files(file1: str, file2: str, output_file: str):\n",
    "    \"\"\"Merge two token files into one, ensuring unique tokens\"\"\"\n",
    "    if not os.path.exists(file1) or not os.path.exists(file2):\n",
    "        raise FileNotFoundError(\"One or both token files do not exist.\")\n",
    "    \n",
    "    tokens = set()\n",
    "    \n",
    "    # Read first file\n",
    "    with open(file1, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens.add(line.strip())\n",
    "    \n",
    "    # Read second file\n",
    "    with open(file2, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens.add(line.strip())\n",
    "    \n",
    "    # Write unique tokens to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for token in sorted(tokens):\n",
    "            f.write(token + '\\n')\n",
    "    \n",
    "    print(f\"Merged {len(tokens)} unique tokens into: {output_file}\")\n",
    "\n",
    "# Merge the two token files\n",
    "merge_token_files(TXT_PATH1, TXT_PATH2, r\"C:\\Users\\Nelso\\Downloads\\merged_spanish_tokens.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb837d6",
   "metadata": {},
   "source": [
    "# Filter words appearing in a common language dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f278e6c",
   "metadata": {},
   "source": [
    "## Filter function v1.0 (prefer v2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8267c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING WITH REAL DATA\n",
      "============================================================\n",
      "Reading tokens from: C:\\Users\\Nelso\\Downloads\\merged_spanish_tokens.txt\n",
      "Loaded 37426 tokens from txt file\n",
      "Reading dictionary from: C:\\Users\\Nelso\\Downloads\\es_dicts\\es\\es_ES.dic\n",
      "Dictionary token count: 58221\n",
      "Loaded 55638 unique tokens from dictionary file (cleaned of Hunspell metadata)\n",
      "Sample removed tokens (case-insensitive matching): Abajo, Abandonar, Abanico, Abdominal, Abel...\n",
      "Removed 11211 tokens that appear in dictionary\n",
      "Remaining tokens: 26215\n",
      "Filtered tokens saved as dictionary to: C:\\Users\\Nelso\\Downloads\\filtered_spanish_tokens.dic\n",
      "\n",
      "Filter result:\n",
      "Original txt tokens: 37426\n",
      "Dictionary tokens: 55638\n",
      "Removed tokens: 11211\n",
      "Remaining tokens: 26215\n",
      "============================================================\n",
      "TESTING CASE-INSENSITIVE MATCHING\n",
      "============================================================\n",
      "Created test files:\n",
      "test_tokens.txt: Abanico, abanico, CASA, Casa, mesa, MESA, Libro, libro\n",
      "test_dict.dic content (after first line): abanico/HS, casa/S, mesa/GS, libro/MS, ...\n",
      "Reading tokens from: test_tokens.txt\n",
      "Loaded 8 tokens from txt file\n",
      "Reading dictionary from: test_dict.dic\n",
      "Dictionary token count: 8\n",
      "Loaded 8 unique tokens from dictionary file (cleaned of Hunspell metadata)\n",
      "Sample removed tokens (case-insensitive matching): Abanico, abanico, CASA, Casa, mesa...\n",
      "Removed 8 tokens that appear in dictionary\n",
      "Remaining tokens: 0\n",
      "Filtered tokens saved as dictionary to: test_output.dic\n",
      "\n",
      "Results:\n",
      "Original txt tokens: 8\n",
      "Dictionary tokens: 8\n",
      "Removed tokens: 8\n",
      "Remaining tokens: 0\n",
      "\n",
      "Output file content:\n",
      "0\n",
      "\n",
      "Expected behavior:\n",
      "- 'Abanico' and 'abanico' should be removed (matches 'abanico/HS')\n",
      "- 'CASA' and 'Casa' should be removed (matches 'casa/S')\n",
      "- 'mesa' and 'MESA' should be removed (matches 'mesa/GS')\n",
      "- 'Libro' and 'libro' should be removed (matches 'libro/MS')\n",
      "- No tokens should remain since all match dictionary entries\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove tokens from a token list based on a common language dictionary (Hunspell)\n",
    "import os  # Add missing import\n",
    "\n",
    "# merged tokens file\n",
    "PATH_Ankama_tokens = r\"C:\\Users\\Nelso\\Downloads\\merged_spanish_tokens.txt\"  # Path to the Ankama tokens file\n",
    "PATH_Hunspell_tokens = r\"C:\\Users\\Nelso\\Downloads\\es_dicts\\es\\es_ES.dic\"  # Path to the Hunspell tokens file\n",
    "\n",
    "\n",
    "def filter_tokens_by_dictionary(txt_file_path: str, dic_file_path: str, output_dic_path: str):\n",
    "    \"\"\"\n",
    "    Read tokens from a txt file and a dic file, remove txt tokens that appear in dic,\n",
    "    then export remaining txt tokens as a dic file with token count\n",
    "    \n",
    "    Args:\n",
    "        txt_file_path: Path to the txt file with tokens (one per line)\n",
    "        dic_file_path: Path to the dic file (first line is token count, rest are tokens)\n",
    "        output_dic_path: Path where the filtered dic file will be saved\n",
    "    \"\"\"\n",
    "    if not os.path.exists(txt_file_path):\n",
    "        raise FileNotFoundError(f\"Token file not found: {txt_file_path}\")\n",
    "    \n",
    "    if not os.path.exists(dic_file_path):\n",
    "        raise FileNotFoundError(f\"Dictionary file not found: {dic_file_path}\")\n",
    "    \n",
    "    # Read tokens from txt file\n",
    "    print(f\"Reading tokens from: {txt_file_path}\")\n",
    "    txt_tokens = []\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token = line.strip()\n",
    "            if token:  # Skip empty lines\n",
    "                txt_tokens.append(token)\n",
    "    \n",
    "    print(f\"Loaded {len(txt_tokens)} tokens from txt file\")\n",
    "    \n",
    "    # Read dictionary file\n",
    "    print(f\"Reading dictionary from: {dic_file_path}\")\n",
    "    with open(dic_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if not lines:\n",
    "        raise ValueError(\"Dictionary file is empty\")\n",
    "    \n",
    "    # First line is the token count (not language code)\n",
    "    original_count = lines[0].strip()\n",
    "    print(f\"Dictionary token count: {original_count}\")\n",
    "    \n",
    "    # Rest are dictionary tokens - clean Hunspell metadata\n",
    "    dic_tokens = set()\n",
    "    for line in lines[1:]:\n",
    "        token = line.strip()\n",
    "        if token:  # Skip empty lines\n",
    "            # Clean Hunspell metadata: remove everything after the slash /\n",
    "            if '/' in token:\n",
    "                token = token.split('/')[0]\n",
    "            dic_tokens.add(token.lower())  # Convert to lowercase for case-insensitive matching\n",
    "    \n",
    "    print(f\"Loaded {len(dic_tokens)} unique tokens from dictionary file (cleaned of Hunspell metadata)\")\n",
    "    \n",
    "    # Filter txt tokens - remove those that appear in dic_tokens (case-insensitive)\n",
    "    filtered_tokens = []\n",
    "    removed_count = 0\n",
    "    sample_removals = []  # Track some examples for demonstration\n",
    "    \n",
    "    for token in txt_tokens:\n",
    "        if token.lower() in dic_tokens:  # Case-insensitive comparison\n",
    "            removed_count += 1\n",
    "            # Store first 10 examples for demonstration\n",
    "            if len(sample_removals) < 10:\n",
    "                sample_removals.append(token)\n",
    "        else:\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    # Show some examples of removed tokens\n",
    "    if sample_removals:\n",
    "        print(f\"Sample removed tokens (case-insensitive matching): {', '.join(sample_removals[:5])}{'...' if len(sample_removals) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"Removed {removed_count} tokens that appear in dictionary\")\n",
    "    print(f\"Remaining tokens: {len(filtered_tokens)}\")\n",
    "    \n",
    "    # Write filtered tokens as dictionary file\n",
    "    with open(output_dic_path, 'w', encoding='utf-8') as f:\n",
    "        # Write token count as first line\n",
    "        f.write(str(len(filtered_tokens)) + '\\n')\n",
    "        # Write remaining tokens\n",
    "        for token in filtered_tokens:\n",
    "            f.write(token + '\\n')\n",
    "    \n",
    "    print(f\"Filtered tokens saved as dictionary to: {output_dic_path}\")\n",
    "    \n",
    "    return {\n",
    "        'original_txt_tokens': len(txt_tokens),\n",
    "        'dic_tokens': len(dic_tokens),\n",
    "        'removed_tokens': removed_count,\n",
    "        'remaining_tokens': len(filtered_tokens)\n",
    "    }\n",
    "\n",
    "def test_case_insensitive_matching():\n",
    "    \"\"\"Test function to demonstrate case-insensitive matching\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING CASE-INSENSITIVE MATCHING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create test files\n",
    "    test_txt_content = \"\"\"Abanico\n",
    "abanico\n",
    "CASA\n",
    "Casa\n",
    "mesa\n",
    "MESA\n",
    "Libro\n",
    "libro\"\"\"\n",
    "    \n",
    "    test_dic_content = \"\"\"8\n",
    "abanico/HS\n",
    "casa/S\n",
    "mesa/GS\n",
    "libro/MS\n",
    "otro/S\n",
    "palabra/S\n",
    "ejemplo/MS\n",
    "test/S\"\"\"\n",
    "    \n",
    "    # Write test files\n",
    "    with open(\"test_tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(test_txt_content)\n",
    "    \n",
    "    with open(\"test_dict.dic\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(test_dic_content)\n",
    "    \n",
    "    print(\"Created test files:\")\n",
    "    print(\"test_tokens.txt:\", test_txt_content.replace('\\n', ', '))\n",
    "    print(\"test_dict.dic content (after first line):\", \"abanico/HS, casa/S, mesa/GS, libro/MS, ...\")\n",
    "    \n",
    "    # Test the function\n",
    "    try:\n",
    "        result = filter_tokens_by_dictionary(\"test_tokens.txt\", \"test_dict.dic\", \"test_output.dic\")\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"Original txt tokens: {result['original_txt_tokens']}\")\n",
    "        print(f\"Dictionary tokens: {result['dic_tokens']}\")\n",
    "        print(f\"Removed tokens: {result['removed_tokens']}\")\n",
    "        print(f\"Remaining tokens: {result['remaining_tokens']}\")\n",
    "        \n",
    "        # Show the output file\n",
    "        with open(\"test_output.dic\", \"r\", encoding=\"utf-8\") as f:\n",
    "            output_content = f.read().strip()\n",
    "        print(f\"\\nOutput file content:\")\n",
    "        print(output_content)\n",
    "        \n",
    "        # Clean up test files\n",
    "        import os\n",
    "        for file in [\"test_tokens.txt\", \"test_dict.dic\", \"test_output.dic\"]:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "        \n",
    "        print(\"\\nExpected behavior:\")\n",
    "        print(\"- 'Abanico' and 'abanico' should be removed (matches 'abanico/HS')\")\n",
    "        print(\"- 'CASA' and 'Casa' should be removed (matches 'casa/S')\")\n",
    "        print(\"- 'mesa' and 'MESA' should be removed (matches 'mesa/GS')\")\n",
    "        print(\"- 'Libro' and 'libro' should be removed (matches 'libro/MS')\")\n",
    "        print(\"- No tokens should remain since all match dictionary entries\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in test: {e}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test function with sample paths\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING WITH REAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    result = filter_tokens_by_dictionary(PATH_Ankama_tokens, PATH_Hunspell_tokens, r\"C:\\Users\\Nelso\\Downloads\\filtered_spanish_tokens.dic\")\n",
    "    \n",
    "    print(\"\\nFilter result:\")\n",
    "    print(f\"Original txt tokens: {result['original_txt_tokens']}\")\n",
    "    print(f\"Dictionary tokens: {result['dic_tokens']}\")\n",
    "    print(f\"Removed tokens: {result['removed_tokens']}\")\n",
    "    print(f\"Remaining tokens: {result['remaining_tokens']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during filtering: {e}\")\n",
    "\n",
    "# Test case-insensitive matching with examples\n",
    "test_case_insensitive_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbf6c8",
   "metadata": {},
   "source": [
    "## Filtering v2.0\n",
    "This new algorithm includes morphological patterns of the AFF files to improve the matching rules and remove more common language words from the Ankama dictionary.\n",
    "* Hunspell resources : https://hunspell.memoq.com/\n",
    "* AFF (affix morphological patterns) documentation : https://manpages.ubuntu.com/manpages/focal/man5/hunspell.5.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c8705d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ENHANCED DICTIONARY FILTERING WITH AFFIX RULES\n",
      "======================================================================\n",
      "Parsing affix rules from: C:\\Users\\Nelso\\Downloads\\es_dicts\\es\\es_ES.aff\n",
      "Loaded 29 prefix flags (80 rules) and 70 suffix flags (6650 rules)\n",
      "Reading tokens from: C:\\Users\\Nelso\\Downloads\\merged_spanish_tokens.txt\n",
      "Loaded 37426 tokens from txt file\n",
      "Reading dictionary and generating word forms from: C:\\Users\\Nelso\\Downloads\\es_dicts\\es\\es_ES.dic\n",
      "Dictionary token count: 58221\n",
      "Processed 1000 dictionary entries...\n",
      "Processed 2000 dictionary entries...\n",
      "Processed 3000 dictionary entries...\n",
      "Processed 4000 dictionary entries...\n",
      "Processed 3000 dictionary entries...\n",
      "Processed 4000 dictionary entries...\n",
      "Processed 5000 dictionary entries...\n",
      "Processed 6000 dictionary entries...\n",
      "Processed 5000 dictionary entries...\n",
      "Processed 6000 dictionary entries...\n",
      "Processed 7000 dictionary entries...\n",
      "Processed 8000 dictionary entries...\n",
      "Processed 9000 dictionary entries...\n",
      "Processed 7000 dictionary entries...\n",
      "Processed 8000 dictionary entries...\n",
      "Processed 9000 dictionary entries...\n",
      "Processed 10000 dictionary entries...\n",
      "Processed 11000 dictionary entries...\n",
      "Processed 12000 dictionary entries...\n",
      "Processed 10000 dictionary entries...\n",
      "Processed 11000 dictionary entries...\n",
      "Processed 12000 dictionary entries...\n",
      "Processed 13000 dictionary entries...\n",
      "Processed 14000 dictionary entries...\n",
      "Processed 13000 dictionary entries...\n",
      "Processed 14000 dictionary entries...\n",
      "Processed 15000 dictionary entries...\n",
      "Processed 16000 dictionary entries...\n",
      "Processed 17000 dictionary entries...\n",
      "Processed 15000 dictionary entries...\n",
      "Processed 16000 dictionary entries...\n",
      "Processed 17000 dictionary entries...\n",
      "Processed 18000 dictionary entries...\n",
      "Processed 18000 dictionary entries...\n",
      "Processed 19000 dictionary entries...\n",
      "Processed 20000 dictionary entries...\n",
      "Processed 19000 dictionary entries...\n",
      "Processed 20000 dictionary entries...\n",
      "Processed 21000 dictionary entries...\n",
      "Processed 22000 dictionary entries...\n",
      "Processed 21000 dictionary entries...\n",
      "Processed 22000 dictionary entries...\n",
      "Processed 23000 dictionary entries...\n",
      "Processed 23000 dictionary entries...\n",
      "Processed 24000 dictionary entries...\n",
      "Processed 25000 dictionary entries...\n",
      "Processed 24000 dictionary entries...\n",
      "Processed 25000 dictionary entries...\n",
      "Processed 26000 dictionary entries...\n",
      "Processed 27000 dictionary entries...\n",
      "Processed 28000 dictionary entries...\n",
      "Processed 29000 dictionary entries...\n",
      "Processed 26000 dictionary entries...\n",
      "Processed 27000 dictionary entries...\n",
      "Processed 28000 dictionary entries...\n",
      "Processed 29000 dictionary entries...\n",
      "Processed 30000 dictionary entries...\n",
      "Processed 31000 dictionary entries...\n",
      "Processed 32000 dictionary entries...\n",
      "Processed 30000 dictionary entries...\n",
      "Processed 31000 dictionary entries...\n",
      "Processed 32000 dictionary entries...\n",
      "Processed 33000 dictionary entries...\n",
      "Processed 34000 dictionary entries...\n",
      "Processed 33000 dictionary entries...\n",
      "Processed 34000 dictionary entries...\n",
      "Processed 35000 dictionary entries...\n",
      "Processed 36000 dictionary entries...\n",
      "Processed 37000 dictionary entries...\n",
      "Processed 35000 dictionary entries...\n",
      "Processed 36000 dictionary entries...\n",
      "Processed 37000 dictionary entries...\n",
      "Processed 38000 dictionary entries...\n",
      "Processed 39000 dictionary entries...\n",
      "Processed 40000 dictionary entries...\n",
      "Processed 38000 dictionary entries...\n",
      "Processed 39000 dictionary entries...\n",
      "Processed 40000 dictionary entries...\n",
      "Processed 41000 dictionary entries...\n",
      "Processed 42000 dictionary entries...\n",
      "Processed 43000 dictionary entries...\n",
      "Processed 41000 dictionary entries...\n",
      "Processed 42000 dictionary entries...\n",
      "Processed 43000 dictionary entries...\n",
      "Processed 44000 dictionary entries...\n",
      "Processed 45000 dictionary entries...\n",
      "Processed 46000 dictionary entries...\n",
      "Processed 44000 dictionary entries...\n",
      "Processed 45000 dictionary entries...\n",
      "Processed 46000 dictionary entries...\n",
      "Processed 47000 dictionary entries...\n",
      "Processed 47000 dictionary entries...\n",
      "Processed 48000 dictionary entries...\n",
      "Processed 49000 dictionary entries...\n",
      "Processed 48000 dictionary entries...\n",
      "Processed 49000 dictionary entries...\n",
      "Processed 50000 dictionary entries...\n",
      "Processed 51000 dictionary entries...\n",
      "Processed 52000 dictionary entries...\n",
      "Processed 50000 dictionary entries...\n",
      "Processed 51000 dictionary entries...\n",
      "Processed 52000 dictionary entries...\n",
      "Processed 53000 dictionary entries...\n",
      "Processed 54000 dictionary entries...\n",
      "Processed 55000 dictionary entries...\n",
      "Processed 53000 dictionary entries...\n",
      "Processed 54000 dictionary entries...\n",
      "Processed 55000 dictionary entries...\n",
      "Processed 56000 dictionary entries...\n",
      "Processed 57000 dictionary entries...\n",
      "Processed 58000 dictionary entries...\n",
      "Generated 644204 unique word forms from 58221 dictionary entries\n",
      "Sample removed tokens: Abajo, Abandonar, Abandonarnos, Abanico, Abdominal...\n",
      "Removed 24394 tokens that match dictionary word forms\n",
      "Remaining tokens: 13032\n",
      "Filtered tokens saved as dictionary to: C:\\Users\\Nelso\\Downloads\\enhanced_filtered_tokens.dic\n",
      "\n",
      "ENHANCED FILTERING RESULTS:\n",
      "==================================================\n",
      "Original txt tokens: 37426\n",
      "Dictionary base words: 58221\n",
      "Generated word forms: 644204\n",
      "Removed tokens: 24394\n",
      "Remaining tokens: 13032\n",
      "Affix expansion factor: 11.06x\n",
      "Additional word forms from affixes: 585983\n",
      "Processed 56000 dictionary entries...\n",
      "Processed 57000 dictionary entries...\n",
      "Processed 58000 dictionary entries...\n",
      "Generated 644204 unique word forms from 58221 dictionary entries\n",
      "Sample removed tokens: Abajo, Abandonar, Abandonarnos, Abanico, Abdominal...\n",
      "Removed 24394 tokens that match dictionary word forms\n",
      "Remaining tokens: 13032\n",
      "Filtered tokens saved as dictionary to: C:\\Users\\Nelso\\Downloads\\enhanced_filtered_tokens.dic\n",
      "\n",
      "ENHANCED FILTERING RESULTS:\n",
      "==================================================\n",
      "Original txt tokens: 37426\n",
      "Dictionary base words: 58221\n",
      "Generated word forms: 644204\n",
      "Removed tokens: 24394\n",
      "Remaining tokens: 13032\n",
      "Affix expansion factor: 11.06x\n",
      "Additional word forms from affixes: 585983\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Set, Dict, List, Tuple\n",
    "\n",
    "def parse_aff_file(aff_file_path: str) -> Dict:\n",
    "    \"\"\"Parse Hunspell .aff file and extract affix rules\"\"\"\n",
    "    affixes = {'PFX': {}, 'SFX': {}}\n",
    "    \n",
    "    with open(aff_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_affix = None\n",
    "    current_type = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue\n",
    "            \n",
    "        # Parse prefix/suffix header definitions (e.g., \"PFX a Y 2\")\n",
    "        if parts[0] in ['PFX', 'SFX'] and len(parts) >= 3:\n",
    "            affix_type = parts[0]\n",
    "            flag = parts[1]\n",
    "            cross_product = parts[2] == 'Y'\n",
    "            \n",
    "            # Check if this is a header line (has count) or rule line\n",
    "            if len(parts) >= 4:\n",
    "                try:\n",
    "                    # Try to parse as count - if successful, this is a header line\n",
    "                    count = int(parts[3])\n",
    "                    # This is a header line\n",
    "                    if flag not in affixes[affix_type]:\n",
    "                        affixes[affix_type][flag] = {\n",
    "                            'cross_product': cross_product,\n",
    "                            'rules': []\n",
    "                        }\n",
    "                    current_affix = flag\n",
    "                    current_type = affix_type\n",
    "                    continue\n",
    "                except ValueError:\n",
    "                    # Not a number, so this is a rule line\n",
    "                    pass\n",
    "            \n",
    "            # Parse affix rule: PFX/SFX flag strip add condition\n",
    "            if len(parts) >= 4 and current_affix == flag and current_type == affix_type:\n",
    "                strip = parts[2] if parts[2] != '0' else ''\n",
    "                add = parts[3] if parts[3] != '0' else ''\n",
    "                condition = parts[4] if len(parts) > 4 else '.'\n",
    "                \n",
    "                if current_affix in affixes[current_type]:\n",
    "                    affixes[current_type][current_affix]['rules'].append({\n",
    "                        'strip': strip,\n",
    "                        'add': add,\n",
    "                        'condition': condition\n",
    "                    })\n",
    "    \n",
    "    return affixes\n",
    "\n",
    "def condition_matches(word: str, condition: str, is_prefix: bool = True) -> bool:\n",
    "    \"\"\"Check if word matches the affix condition pattern\"\"\"\n",
    "    if condition == '.':\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        if is_prefix:\n",
    "            # For prefixes, check the beginning of the word\n",
    "            return bool(re.match(f'^{condition}', word))\n",
    "        else:\n",
    "            # For suffixes, check the end of the word\n",
    "            return bool(re.search(f'{condition}$', word))\n",
    "    except re.error:\n",
    "        # If regex fails, do simple string matching\n",
    "        if is_prefix:\n",
    "            return word.startswith(condition.replace('[^', '').replace(']', ''))\n",
    "        else:\n",
    "            return word.endswith(condition.replace('[^', '').replace(']', ''))\n",
    "\n",
    "def generate_word_forms(base_word: str, flags: str, affixes: Dict) -> Set[str]:\n",
    "    \"\"\"Generate all possible word forms using affix rules\"\"\"\n",
    "    word_forms = {base_word}  # Always include the base word\n",
    "    \n",
    "    if not flags:\n",
    "        return word_forms\n",
    "    \n",
    "    # Process each flag character\n",
    "    for flag in flags:\n",
    "        # Apply prefixes\n",
    "        if flag in affixes['PFX']:\n",
    "            prefix_rules = affixes['PFX'][flag]['rules']\n",
    "            for rule in prefix_rules:\n",
    "                if condition_matches(base_word, rule['condition'], is_prefix=True):\n",
    "                    # Apply prefix rule\n",
    "                    if rule['strip']:\n",
    "                        if base_word.startswith(rule['strip']):\n",
    "                            modified_word = rule['add'] + base_word[len(rule['strip']):]\n",
    "                            word_forms.add(modified_word)\n",
    "                    else:\n",
    "                        modified_word = rule['add'] + base_word\n",
    "                        word_forms.add(modified_word)\n",
    "        \n",
    "        # Apply suffixes\n",
    "        if flag in affixes['SFX']:\n",
    "            suffix_rules = affixes['SFX'][flag]['rules']\n",
    "            for rule in suffix_rules:\n",
    "                if condition_matches(base_word, rule['condition'], is_prefix=False):\n",
    "                    # Apply suffix rule\n",
    "                    if rule['strip']:\n",
    "                        if base_word.endswith(rule['strip']):\n",
    "                            modified_word = base_word[:-len(rule['strip'])] + rule['add']\n",
    "                            word_forms.add(modified_word)\n",
    "                    else:\n",
    "                        modified_word = base_word + rule['add']\n",
    "                        word_forms.add(modified_word)\n",
    "    \n",
    "    return word_forms\n",
    "\n",
    "def filter_tokens_by_dictionary_with_affixes(txt_file_path: str, dic_file_path: str, aff_file_path: str, output_dic_path: str):\n",
    "    \"\"\"\n",
    "    Enhanced version that uses Hunspell affix rules for better matching\n",
    "    \n",
    "    Args:\n",
    "        txt_file_path: Path to the txt file with tokens (one per line)\n",
    "        dic_file_path: Path to the dic file (first line is token count, rest are tokens)\n",
    "        aff_file_path: Path to the .aff file with affix rules\n",
    "        output_dic_path: Path where the filtered dic file will be saved\n",
    "    \"\"\"\n",
    "    if not os.path.exists(txt_file_path):\n",
    "        raise FileNotFoundError(f\"Token file not found: {txt_file_path}\")\n",
    "    \n",
    "    if not os.path.exists(dic_file_path):\n",
    "        raise FileNotFoundError(f\"Dictionary file not found: {dic_file_path}\")\n",
    "        \n",
    "    if not os.path.exists(aff_file_path):\n",
    "        raise FileNotFoundError(f\"Affix file not found: {aff_file_path}\")\n",
    "    \n",
    "    # Parse affix rules\n",
    "    print(f\"Parsing affix rules from: {aff_file_path}\")\n",
    "    affixes = parse_aff_file(aff_file_path)\n",
    "    prefix_count = sum(len(rules['rules']) for rules in affixes['PFX'].values())\n",
    "    suffix_count = sum(len(rules['rules']) for rules in affixes['SFX'].values())\n",
    "    print(f\"Loaded {len(affixes['PFX'])} prefix flags ({prefix_count} rules) and {len(affixes['SFX'])} suffix flags ({suffix_count} rules)\")\n",
    "    \n",
    "    # Read tokens from txt file - preserve original case\n",
    "    print(f\"Reading tokens from: {txt_file_path}\")\n",
    "    original_txt_tokens = []  # Keep original case\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            token = line.strip()\n",
    "            if token:\n",
    "                original_txt_tokens.append(token)  # Preserve original case\n",
    "    \n",
    "    print(f\"Loaded {len(original_txt_tokens)} tokens from txt file\")\n",
    "    \n",
    "    # Read dictionary file and generate all word forms\n",
    "    print(f\"Reading dictionary and generating word forms from: {dic_file_path}\")\n",
    "    with open(dic_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if not lines:\n",
    "        raise ValueError(\"Dictionary file is empty\")\n",
    "    \n",
    "    # First line is the token count\n",
    "    original_count = lines[0].strip()\n",
    "    print(f\"Dictionary token count: {original_count}\")\n",
    "    \n",
    "    # Generate all possible word forms from dictionary (in lowercase for matching)\n",
    "    all_dictionary_forms = set()\n",
    "    processed_entries = 0\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        processed_entries += 1\n",
    "        if processed_entries % 1000 == 0:\n",
    "            print(f\"Processed {processed_entries} dictionary entries...\")\n",
    "        \n",
    "        # Parse dictionary entry\n",
    "        if '/' in line:\n",
    "            base_word, flags = line.split('/', 1)\n",
    "        else:\n",
    "            base_word, flags = line, ''\n",
    "        \n",
    "        # Generate all word forms for this base word (lowercase for matching)\n",
    "        word_forms = generate_word_forms(base_word.lower(), flags, affixes)\n",
    "        all_dictionary_forms.update(word_forms)\n",
    "    \n",
    "    print(f\"Generated {len(all_dictionary_forms)} unique word forms from {processed_entries} dictionary entries\")\n",
    "    \n",
    "    # Filter txt tokens - remove those that match any dictionary form\n",
    "    # Compare lowercase versions but keep original case for output\n",
    "    filtered_tokens = []\n",
    "    removed_count = 0\n",
    "    sample_removals = []\n",
    "    \n",
    "    for original_token in original_txt_tokens:  # Use original case tokens\n",
    "        if original_token.lower() in all_dictionary_forms:  # Compare with lowercase\n",
    "            removed_count += 1\n",
    "            if len(sample_removals) < 10:\n",
    "                sample_removals.append(original_token)  # Show original case in samples\n",
    "        else:\n",
    "            filtered_tokens.append(original_token)  # Keep original case\n",
    "    \n",
    "    # Show some examples of removed tokens\n",
    "    if sample_removals:\n",
    "        print(f\"Sample removed tokens: {', '.join(sample_removals[:5])}{'...' if len(sample_removals) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"Removed {removed_count} tokens that match dictionary word forms\")\n",
    "    print(f\"Remaining tokens: {len(filtered_tokens)}\")\n",
    "    \n",
    "    # Write filtered tokens as dictionary file (preserving original case)\n",
    "    with open(output_dic_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(len(filtered_tokens)) + '\\n')\n",
    "        for token in filtered_tokens:  # These already have original case\n",
    "            f.write(token + '\\n')\n",
    "    \n",
    "    print(f\"Filtered tokens saved as dictionary to: {output_dic_path}\")\n",
    "    \n",
    "    return {\n",
    "        'original_txt_tokens': len(original_txt_tokens),\n",
    "        'dictionary_base_words': processed_entries,\n",
    "        'generated_word_forms': len(all_dictionary_forms),\n",
    "        'removed_tokens': removed_count,\n",
    "        'remaining_tokens': len(filtered_tokens)\n",
    "    }\n",
    "\n",
    "# Test the enhanced function\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING ENHANCED DICTIONARY FILTERING WITH AFFIX RULES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example usage\n",
    "AFF_FILE_PATH = r\"C:\\Users\\Nelso\\Downloads\\es_dicts\\es\\es_ES.aff\"  # Path to .aff file\n",
    "\n",
    "if os.path.exists(AFF_FILE_PATH):\n",
    "    try:\n",
    "        result = filter_tokens_by_dictionary_with_affixes(\n",
    "            PATH_Ankama_tokens,      # txt file with tokens to filter\n",
    "            PATH_Hunspell_tokens,    # dic file\n",
    "            AFF_FILE_PATH,           # aff file with rules\n",
    "            r\"C:\\Users\\Nelso\\Downloads\\enhanced_filtered_tokens.dic\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nENHANCED FILTERING RESULTS:\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Original txt tokens: {result['original_txt_tokens']}\")\n",
    "        print(f\"Dictionary base words: {result['dictionary_base_words']}\")\n",
    "        print(f\"Generated word forms: {result['generated_word_forms']}\")\n",
    "        print(f\"Removed tokens: {result['removed_tokens']}\")\n",
    "        print(f\"Remaining tokens: {result['remaining_tokens']}\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = result['generated_word_forms'] - result['dictionary_base_words']\n",
    "        print(f\"Affix expansion factor: {result['generated_word_forms'] / result['dictionary_base_words']:.2f}x\")\n",
    "        print(f\"Additional word forms from affixes: {improvement}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"Affix file not found: {AFF_FILE_PATH}\")\n",
    "    print(\"Please provide the correct path to the .aff file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b92cf0",
   "metadata": {},
   "source": [
    "# Enhanced Language File Processor - Complete Summary\n",
    "\n",
    "## Features\n",
    "\n",
    "The script now includes **comprehensive filtering** with multiple advanced conditions to ensure high-quality token extraction.\n",
    "\n",
    "### Supported File Types\n",
    "- **Excel files** (`.xlsx`, `.xls`): Language code as column name\n",
    "- **XLIFF files** (`.xliff`, `.xlf`, `.xml`): Language code in `source-language` or `target-language` attributes\n",
    "\n",
    "### Key Functionality\n",
    "1. **File Type Detection**: Automatically detects file type based on extension\n",
    "2. **Language Matching**: \n",
    "   - Excel: Extracts from column matching the language code\n",
    "   - XLIFF: Extracts from `<source>` or `<target>` elements based on language attributes\n",
    "\n",
    "### **COMPREHENSIVE Filtering System**\n",
    "3. **Square Bracket Filtering**: Ignores entries where source text contains `[.+]` pattern\n",
    "4. **Target = Source Filtering**: Ignores entries where target text equals source text\n",
    "5. **All-Caps Target Filtering**: **NEW** - Ignores entries where target text is entirely in uppercase\n",
    "6. **HTML Tag Removal**: **NEW** - Removes HTML tags and decodes HTML entities before tokenization\n",
    "7. **Hyperlink & Email Removal**: Removes URLs and email addresses before tokenization\n",
    "8. **Token Edge Cleaning**: **NEW** - Removes leading/trailing apostrophes and hyphens from tokens\n",
    "9. **Short Token Filtering**: Removes tokens with length < 3 characters\n",
    "10. **Same Character Chain Filtering**: Removes tokens that are chains of the same character (e.g., \"aaa\", \"zzZZzz\")\n",
    "11. **Number-Only Token Filtering**: **NEW** - Removes tokens that consist only of digits\n",
    "12. **Time Pattern Filtering**: **NEW** - Removes tokens matching `\\d+(PA|PM|AM|AL)` pattern\n",
    "13. **Digit-Word Pattern Filtering**: **NEW** - Removes tokens matching `\\d+-\\w+` pattern (e.g., \"123-neutral\")\n",
    "14. **Enhanced Punctuation**: **NEW** - Includes º character in punctuation list\n",
    "15. **Tokenization**: Splits by whitespace and punctuation, preserving hyphens (`-`) and apostrophes (`'`)\n",
    "16. **Export**: Saves unique tokens (case-sensitive) to text file, one per line\n",
    "\n",
    "### Usage\n",
    "```python\n",
    "# Basic usage\n",
    "tokens = process_file(file_path, language_code)\n",
    "\n",
    "# With custom output path\n",
    "tokens = process_file(file_path, language_code, output_path)\n",
    "```\n",
    "\n",
    "### Example Advanced Filtering Results\n",
    "**Input Processing:**\n",
    "- ✅ **\"Hola mundo\"** → `['Hola', 'mundo']`\n",
    "- ❌ **\"[Debug] test\"** → Skipped (square brackets in source)\n",
    "- ❌ **\"Same text\"** → Skipped (target equals source)\n",
    "- ❌ **\"TODO EN MAYÚSCULAS\"** → Skipped (all caps target)\n",
    "- ✅ **HTML content** → Tags removed, entities decoded\n",
    "- ✅ **\"'Resistencia 'Robo'\"** → `['Resistencia', 'Robo']` (edges cleaned)\n",
    "- ❌ **Number tokens: \"123\", \"456\"** → Filtered out (numbers only)\n",
    "- ❌ **Time patterns: \"3PM\", \"10AM\"** → Filtered out (time pattern)\n",
    "- ❌ **Digit-word: \"123-neutral\"** → Filtered out (digit-word pattern)\n",
    "- ✅ **\"25º celsius\"** → `['celsius']` (º treated as punctuation)\n",
    "\n",
    "**Final Result:** Only meaningful, clean tokens ≥ 3 characters from appropriate entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5b584",
   "metadata": {},
   "source": [
    "# Draft tests (unitary tests TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification test for the fixes\n",
    "print(\"=\"*50)\n",
    "print(\"VERIFYING FIXES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test the corrected time pattern function\n",
    "test_time_tokens = [\"3PM\", \"10AM\", \"5PA\", \"12AL\"]\n",
    "print(\"Testing corrected time patterns:\")\n",
    "for token in test_time_tokens:\n",
    "    matches = matches_time_pattern(token)\n",
    "    print(f\"'{token}' -> matches time pattern: {matches}\")\n",
    "\n",
    "# Test º character removal\n",
    "test_text = \"Temperature: 25º celsius\"\n",
    "print(f\"\\nTesting º removal:\")\n",
    "print(f\"Original: '{test_text}'\")\n",
    "tokens = tokenize_text(test_text)\n",
    "print(f\"Tokens: {sorted(tokens)}\")\n",
    "\n",
    "# Test all filtering combined\n",
    "test_combined_text = \"Meeting at 3PM, temperature 25º, status: 123-neutral, numbers 456\"\n",
    "print(f\"\\nTesting combined filtering:\")\n",
    "print(f\"Original: '{test_combined_text}'\")\n",
    "tokens_combined = tokenize_text(test_combined_text)\n",
    "print(f\"Final tokens: {sorted(tokens_combined)}\")\n",
    "\n",
    "print(\"\\nExpected results:\")\n",
    "print(\"- Time patterns (3PM) should be filtered out\")\n",
    "print(\"- Numbers (456) should be filtered out\") \n",
    "print(\"- Digit-word patterns (123-neutral) should be filtered out\")\n",
    "print(\"- º should be treated as punctuation\")\n",
    "print(\"- Only meaningful words should remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2967cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fix for HTML br and p tag handling\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING HTML BR AND P TAG HANDLING FIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test cases that demonstrate the issue and fix\n",
    "test_html_cases = [\n",
    "    \"Ankama&lt;br&gt;&lt;br&gt;1.\",\n",
    "    \"Word1&lt;br&gt;Word2\",\n",
    "    \"Start&lt;p&gt;Middle&lt;/p&gt;End\",\n",
    "    \"Text&lt;br/&gt;More text\",\n",
    "    \"Line1&lt;BR&gt;Line2\",  # Test case insensitive\n",
    "    \"Para&lt;P class='test'&gt;Content&lt;/P&gt;After\",\n",
    "    \"Normal text without HTML tags\"\n",
    "]\n",
    "\n",
    "print(\"Testing HTML tag removal with br/p handling:\")\n",
    "for text in test_html_cases:\n",
    "    cleaned = remove_html_tags(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    print(f\"Original: '{text}'\")\n",
    "    print(f\"Cleaned:  '{cleaned}'\")\n",
    "    print(f\"Tokens:   {sorted(tokens)}\")\n",
    "    print()\n",
    "\n",
    "# Specific test for the reported issue\n",
    "print(\"=\"*40)\n",
    "print(\"SPECIFIC TEST FOR REPORTED ISSUE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "issue_text = \"Ankama&lt;br&gt;&lt;br&gt;1.\"\n",
    "print(f\"Testing: '{issue_text}'\")\n",
    "\n",
    "# Before fix (simulate): would result in \"Ankama1\"\n",
    "# After fix: should result in separate tokens\n",
    "cleaned_text = remove_html_tags(issue_text)\n",
    "final_tokens = tokenize_text(issue_text)\n",
    "\n",
    "print(f\"HTML removed: '{cleaned_text}'\")\n",
    "print(f\"Final tokens: {sorted(final_tokens)}\")\n",
    "print(f\"✅ Issue fixed: 'Ankama' and other meaningful tokens are separate\" if 'Ankama' in final_tokens else \"❌ Issue not fixed\")\n",
    "\n",
    "# Test with a more complex example\n",
    "complex_html = \"Company&lt;br&gt;&lt;br&gt;Address&lt;p&gt;City&lt;/p&gt;Country123\"\n",
    "print(f\"\\nComplex example: '{complex_html}'\")\n",
    "complex_tokens = tokenize_text(complex_html)\n",
    "print(f\"Tokens: {sorted(complex_tokens)}\")\n",
    "print(\"Expected: Company, Address, City, Country123 should be separate tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new ignore_identical_translation parameter\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING ignore_identical_translation PARAMETER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test data with identical translations\n",
    "test_data_identical = {\n",
    "    'key': ['greeting', 'same1', 'same2', 'different'],\n",
    "    'fr-fr': ['Bonjour', 'Same Text', 'Identical', 'Source Text'],\n",
    "    'es-es': ['Hola', 'Same Text', 'Identical', 'Target Text']  # First two are identical to source\n",
    "}\n",
    "\n",
    "df_identical = pd.DataFrame(test_data_identical)\n",
    "df_identical.to_excel(\"test_identical.xlsx\", index=False)\n",
    "print(\"Test Excel file with identical translations created!\")\n",
    "print(\"Test data:\")\n",
    "print(df_identical.to_string(index=False))\n",
    "\n",
    "# Test with ignore_identical_translation=True (default)\n",
    "print(f\"\\n1. Testing with ignore_identical_translation=True (default):\")\n",
    "try:\n",
    "    tokens_ignore_true = process_file(\"test_identical.xlsx\", \"es-es\", \"tokens_ignore_true.txt\")\n",
    "    print(f\"Tokens with ignore=True: {sorted(tokens_ignore_true)}\")\n",
    "    print(\"Expected: 'Same Text' and 'Identical' should be skipped\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with ignore_identical_translation=False\n",
    "print(f\"\\n2. Testing with ignore_identical_translation=False:\")\n",
    "try:\n",
    "    tokens_ignore_false = process_file(\"test_identical.xlsx\", \"es-es\", \"tokens_ignore_false.txt\", ignore_identical_translation=False)\n",
    "    print(f\"Tokens with ignore=False: {sorted(tokens_ignore_false)}\")\n",
    "    print(\"Expected: 'Same Text' and 'Identical' should be included\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Show the difference\n",
    "if 'tokens_ignore_true' in locals() and 'tokens_ignore_false' in locals():\n",
    "    additional_tokens = tokens_ignore_false - tokens_ignore_true\n",
    "    print(f\"\\nAdditional tokens when ignore_identical_translation=False: {sorted(additional_tokens)}\")\n",
    "\n",
    "# Also test with XLIFF\n",
    "test_xliff_identical = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<xliff version=\"1.2\" xmlns=\"urn:oasis:names:tc:xliff:document:1.2\">\n",
    "    <file datatype=\"plaintext\" original=\"test\" source-language=\"fr-fr\" target-language=\"es-es\">\n",
    "        <body>\n",
    "            <trans-unit id=\"test.1\">\n",
    "                <source>Hello World</source>\n",
    "                <target>Hola Mundo</target>\n",
    "            </trans-unit>\n",
    "            <trans-unit id=\"test.2\">\n",
    "                <source>Same Text</source>\n",
    "                <target>Same Text</target>\n",
    "            </trans-unit>\n",
    "            <trans-unit id=\"test.3\">\n",
    "                <source>Identical</source>\n",
    "                <target>Identical</target>\n",
    "            </trans-unit>\n",
    "        </body>\n",
    "    </file>\n",
    "</xliff>\"\"\"\n",
    "\n",
    "with open(\"test_identical.xliff\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_xliff_identical)\n",
    "\n",
    "print(f\"\\n3. Testing XLIFF with ignore_identical_translation=True:\")\n",
    "try:\n",
    "    xliff_tokens_true = process_file(\"test_identical.xliff\", \"es-es\", \"xliff_tokens_true.txt\")\n",
    "    print(f\"XLIFF tokens with ignore=True: {sorted(xliff_tokens_true)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\n4. Testing XLIFF with ignore_identical_translation=False:\")\n",
    "try:\n",
    "    xliff_tokens_false = process_file(\"test_identical.xliff\", \"es-es\", \"xliff_tokens_false.txt\", ignore_identical_translation=False)\n",
    "    print(f\"XLIFF tokens with ignore=False: {sorted(xliff_tokens_false)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Clean up test files\n",
    "print(\"\\nCleaning up test files...\")\n",
    "test_files = [\n",
    "    \"test_identical.xlsx\", \"test_identical.xliff\",\n",
    "    \"tokens_ignore_true.txt\", \"tokens_ignore_false.txt\",\n",
    "    \"xliff_tokens_true.txt\", \"xliff_tokens_false.txt\"\n",
    "]\n",
    "for file in test_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Removed: {file}\")\n",
    "\n",
    "print(\"\\nParameter test completed!\")\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"- ignore_identical_translation=True (default): Skips entries where target equals source\")\n",
    "print(\"- ignore_identical_translation=False: Includes all entries, even identical translations\")\n",
    "print(\"- This allows users to control whether to include identical translations in their token extraction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
